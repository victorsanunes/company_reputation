{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análise de sentimentos sobre linhas aéreas (Parte 2/4)\n",
    "\n",
    "Este notebook faz parte de um conjunto de notebooks com o objetivo de realizar a tarefa de análise de sentimentos em tweets sobre linhas aéreas americanas. \n",
    "\n",
    "Neste segundo notebook, iremos treinar alguns modelos de machine learning. A base utilizada para treino se encontra [aqui](https://www.kaggle.com/crowdflower/twitter-airline-sentiment). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variavel que indica se queremos salvar um modelo treinando no HD\n",
    "SAVE_MODEL_TO_DISK = 1\n",
    "\n",
    "# Variavel que indica se queremos carregar um modelo salvo no HD\n",
    "LOAD_MODEL = 0\n",
    "\n",
    "# GENERAL LIBS\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "%matplotlib inline\n",
    "\n",
    "# SKLEARN\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "#NLTK\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# GENSIM\n",
    "from gensim.sklearn_api import W2VTransformer\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetsDF = pd.read_csv(\"preprocessed2.csv\")\n",
    "\n",
    "tweetsDF.drop(labels=[\"Unnamed: 0\",\n",
    "                      \"airline\", \n",
    "                      \"negativereason\", \n",
    "                      \"airline_sentiment_confidence\", \n",
    "                      \"negativereason\",\n",
    "                      \"negativereason_confidence\",\n",
    "                      \"airline_sentiment\",\n",
    "                      \"text\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(tokens):\n",
    "    '''\n",
    "    Apply stemming to each token\n",
    "    \n",
    "    @return:\n",
    "        Return a list of stemmed tokens\n",
    "    '''\n",
    "    \n",
    "    stemmer = SnowballStemmer(\"english\")  \n",
    "    stemmed = [stemmer.stem(w) for w in tokens.split()]\n",
    "    return stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetsDF[\"tweet2words\"] = tweetsDF[\"tweet2words\"].values.astype(\"U\")\n",
    "tweetsDF[\"correctedText\"] = tweetsDF[\"correctedText\"].values.astype(\"U\")\n",
    "tweetsDF.drop(labels=['tweet2words'], axis=1, inplace=True)\n",
    "\n",
    "# tweetsDF[\"correctedText\"] = tweetsDF[\"correctedText\"].apply(stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>num_capitalized</th>\n",
       "      <th>tweet_length</th>\n",
       "      <th>num_negative_words</th>\n",
       "      <th>num_positive_words</th>\n",
       "      <th>num_neutral_words</th>\n",
       "      <th>has_capitalized</th>\n",
       "      <th>num_capitalised_positive_words</th>\n",
       "      <th>num_capitalised_negative_words</th>\n",
       "      <th>num_hashtags</th>\n",
       "      <th>num_special_character</th>\n",
       "      <th>correctedText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>What said</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>plus added commercials experience tacky</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment  num_capitalized  tweet_length  num_negative_words  \\\n",
       "0          1                0             3                   0   \n",
       "1          1                0             6                   0   \n",
       "\n",
       "   num_positive_words  num_neutral_words  has_capitalized  \\\n",
       "0                   0                  4                1   \n",
       "1                   0                  9                0   \n",
       "\n",
       "   num_capitalised_positive_words  num_capitalised_negative_words  \\\n",
       "0                               0                               0   \n",
       "1                               0                               0   \n",
       "\n",
       "   num_hashtags  num_special_character  \\\n",
       "0             0                      3   \n",
       "1             0                      4   \n",
       "\n",
       "                              correctedText  \n",
       "0                                 What said  \n",
       "1   plus added commercials experience tacky  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweetsDF.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"sentiment\"\n",
    "features = [c for c in tweetsDF.columns.values if c not in [target]]\n",
    "numeric_features =  [c for c in tweetsDF.columns.values if c not in ['tweet2words', 'correctedText', target]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, Y_train, Y_test = train_test_split(tweetsDF[features], tweetsDF[target], test_size=0.33, random_state=42)\n",
    "X_train = tweetsDF[features]\n",
    "Y_train = tweetsDF[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' seriously would pay flight seats playing really bad thing flying VA'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train['correctedText'][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (14640, 11)\n",
      "Y_train: (14640,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train: \" + repr(X_train.shape))\n",
    "# print(\"X_test: \" + repr(X_test.shape))\n",
    "print(\"Y_train: \" + repr(Y_train.shape))\n",
    "# print(\"Y_test: \" + repr(Y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "A criação dos pipelines foi feita utilizando os seguintes kernels no kaggle como auxiliadores;\n",
    "1. [Building A Scikit Learn Classification Pipeline](https://www.kaggle.com/gautham11/building-a-scikit-learn-classification-pipeline)\n",
    "2. [A Deep Dive Into Sklearn Pipelines](https://www.kaggle.com/baghern/a-deep-dive-into-sklearn-pipelines)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transformer to select a single column from the data frame to perform additional transformations on\n",
    "    Use on text columns in the data\n",
    "    \"\"\"\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def splitString(self, s):\n",
    "        try:\n",
    "            return s.split()\n",
    "        except AttributeError:\n",
    "            return \"\"\n",
    "            \n",
    "    \n",
    "    def transform(self, X):\n",
    "        # Apply the word2vec transformation\n",
    "        a = X[self.key]\n",
    "#         return wordvecs.fit_transform(a)\n",
    "        return a\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Senteces(object):\n",
    "    def __init__(self, dirname):\n",
    "        self.dirname = dirname\n",
    " \n",
    "    def __iter__(self):\n",
    "        for fname in os.listdir(self.dirname):\n",
    "            for line in open(os.path.join(self.dirname, fname)):\n",
    "                yield line.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumberSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transformer to select a single column from the data frame to perform additional transformations on\n",
    "    Use on numeric columns in the data\n",
    "    \"\"\"\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[[self.key]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = Pipeline([\n",
    "                ('selector', TextSelector(key='correctedText')),\n",
    "                ('countVec', CountVectorizer(analyzer = \"word\"))\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "numCapitalized = Pipeline([\n",
    "                    ('selector', NumberSelector(key='num_capitalized')),\n",
    "#                     ('standard', StandardScaler())\n",
    "                    ('standard', MinMaxScaler())\n",
    "                ])\n",
    "\n",
    "tweetLength = Pipeline([\n",
    "                    ('selector', NumberSelector(key='tweet_length')),\n",
    "#                     ('standard', StandardScaler())\n",
    "                    ('standard', MinMaxScaler())\n",
    "                ])\n",
    "\n",
    "numNegativeWords = Pipeline([\n",
    "                    ('selector', NumberSelector(key='num_negative_words')),\n",
    "                    #('standard', StandardScaler())\n",
    "                    ('standard', MinMaxScaler())\n",
    "                ])\n",
    "\n",
    "numPositiveWords = Pipeline([\n",
    "                    ('selector', NumberSelector(key='num_positive_words')),\n",
    "#                     ('standard', StandardScaler())\n",
    "                    ('standard', MinMaxScaler())\n",
    "                ])\n",
    "\n",
    "numNeutralWords = Pipeline([\n",
    "                    ('selector', NumberSelector(key='num_neutral_words')),\n",
    "#                     ('standard', StandardScaler())\n",
    "                    ('standard', MinMaxScaler())\n",
    "                ])\n",
    "\n",
    "\n",
    "numCapitalizedPositiveWords = Pipeline([\n",
    "                    ('selector', NumberSelector(key='num_capitalised_positive_words')),\n",
    "#                     ('standard', StandardScaler())\n",
    "                    ('standard', MinMaxScaler())\n",
    "                ])\n",
    "\n",
    "numCapitalizedNegativeWords = Pipeline([\n",
    "                    ('selector', NumberSelector(key='num_capitalised_negative_words')),\n",
    "#                     ('standard', StandardScaler())\n",
    "                    ('standard', MinMaxScaler())\n",
    "                ])\n",
    "\n",
    "numHashtags = Pipeline([\n",
    "                    ('selector', NumberSelector(key='num_hashtags')),\n",
    "#                     ('standard', StandardScaler())\n",
    "                    ('standard', MinMaxScaler())\n",
    "                ])\n",
    "\n",
    "\n",
    "numSpecialCharacter = Pipeline([\n",
    "                    ('selector', NumberSelector(key='num_special_character')),\n",
    "#                     ('standard', StandardScaler())\n",
    "                    ('standard', MinMaxScaler())\n",
    "                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<14640x9769 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 190942 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats = FeatureUnion([('text', text),\n",
    "                      ('numCapitalized', numCapitalized),\n",
    "                      (\"tweetLength\", tweetLength),\n",
    "                      (\"numNegativeWords\", numNegativeWords),\n",
    "                      (\"numPositiveWords\", numPositiveWords),\n",
    "                      (\"numNeutralWords\", numNeutralWords),\n",
    "                      (\"numCapitalizedPositiveWords\", numCapitalizedPositiveWords),\n",
    "                      (\"numCapitalizedNegativeWords\", numCapitalizedNegativeWords),\n",
    "                      (\"numHashtags\", numHashtags),\n",
    "                      (\"numSpecialCharacter\", numSpecialCharacter)\n",
    "                     ])\n",
    "\n",
    "feature_processing = Pipeline([('feats', feats)])\n",
    "feature_processing.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('features', FeatureUnion(n_jobs=None,\n",
       "       transformer_list=[('text', Pipeline(memory=None,\n",
       "     steps=[('selector', TextSelector(key='correctedText')), ('countVec', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8',...ators=200, n_jobs=None,\n",
       "            oob_score=False, random_state=42, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features',feats),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=200, random_state = 42)),\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
      "  kernel='linear', max_iter=-1, probability=False, random_state=None,\n",
      "  shrinking=True, tol=0.001, verbose=False)\n",
      "-----------------------------------\n",
      "---------------------------------\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "-----------------------------------\n",
      "---------------------------------\n",
      "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
      "       random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
      "       validation_fraction=0.1, verbose=False, warm_start=False)\n",
      "-----------------------------------\n",
      "---------------------------------\n",
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "if LOAD_MODEL == 0:\n",
    "    clfs = list()\n",
    "    \n",
    "    clfs.append(SVC(kernel='linear'))\n",
    "    clfs.append(DecisionTreeClassifier())\n",
    "    clfs.append(MLPClassifier())\n",
    "    clfs.append(MultinomialNB())\n",
    "\n",
    "    scores = list()\n",
    "    pipelineList = list()\n",
    "\n",
    "    for c in clfs:\n",
    "        pipeline.set_params(classifier = c)\n",
    "        pipeline.fit(X_train, Y_train)\n",
    "        s = cross_validate(pipeline, X_train, Y_train, \n",
    "                           scoring=[\"accuracy\", \"recall\", \"precision\", \"f1\"], \n",
    "                           cv=10, return_estimator = True)\n",
    "        scores.append(s)\n",
    "        pipelineList.append(pipeline)\n",
    "        \n",
    "        print('---------------------------------')\n",
    "        print(str(c))\n",
    "        print('-----------------------------------')\n",
    "#         for key, values in s.items():\n",
    "#                 print(key,' mean ', values.mean())\n",
    "#                 print(key,' std ', values.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestModels = list()\n",
    "for model in scores:\n",
    "    maxAcc = max(model['test_accuracy'])\n",
    "    bestModelX = np.where(model['test_accuracy'] == maxAcc)\n",
    "    bestModels.append(model['estimator'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC\n",
      "DecisionTreeClassifier\n",
      "MLPClassifier\n",
      "MultinomialNB\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(bestModels)):\n",
    "    name = bestModels[i].get_params('classifier')\n",
    "    print(str(name['classifier']).split('(')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm = scores[0]\n",
    "# maxAcc = max(svm['test_accuracy'])\n",
    "# clf = svm['estimator'][np.where(model['test_accuracy'] == maxAcc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SVC.joblib',\n",
       " 'MLPClassifier.joblib',\n",
       " 'DecisionTreeClassifier.joblib',\n",
       " 'MultinomialNB.joblib']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "modelsPath = 'models/'\n",
    "onlyfiles = [f for f in listdir(modelsPath) if isfile(join(modelsPath, f))]\n",
    "onlyfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAVE_MODEL_TO_DISK == 1:\n",
    "#     for i in range(len(pipelineList)):\n",
    "#         joblib.dump(pipelineList[i], \"model[\" +str(i)+ \"].joblib\") \n",
    "    for i in range(len(bestModels)):\n",
    "        model = bestModels[i].get_params('classifier')\n",
    "        file = str(model['classifier']).split('(')[0]\n",
    "        joblib.dump(bestModels[i], file + \".joblib\") \n",
    "elif LOAD_MODEL == 1:\n",
    "    bestModels = list()\n",
    "    for file in onlyfiles:\n",
    "        bestModels.append(joblib.load(modelsPath + file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC\n",
      "DecisionTreeClassifier\n",
      "MLPClassifier\n",
      "MultinomialNB\n"
     ]
    }
   ],
   "source": [
    "for model in bestModels:\n",
    "    print(str(model.get_params('classifier')['classifier']).split('(')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora que temos os melhores modelos treinados, vamos usá-los para classificar os tweets coletados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "tweetsPath = '03_processed/'\n",
    "onlyfiles = [f for f in listdir(tweetsPath) if isfile(join(tweetsPath, f))]\n",
    "# model = bestModels[0]\n",
    "for model in bestModels: \n",
    "    for file in onlyfiles:\n",
    "        newTweets = pd.read_csv(tweetsPath + file)\n",
    "        newTweets.drop(labels=['text', 'tweet2words'], axis = 1, inplace=True)\n",
    "        newTweets['correctedText'] = newTweets[\"correctedText\"].values.astype(\"U\")\n",
    "        newTweets['sentiment'] = model.predict(newTweets)\n",
    "        classifierName = str(model.get_params('classifier')['classifier']).split('(')[0]\n",
    "#         newTweets.to_csv('04_output/' + classifierName + '/' + file.split('.')[0] + '_prediction.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(tweetsDF[features], tweetsDF[target], test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# bestModel = clf[0]['estimator'][0]\n",
    "allMeasures = dict()\n",
    "for i in range(len(bestModels)):\n",
    "    model = bestModels[i]\n",
    "    tn, fp, fn, tp = confusion_matrix(model.predict(X_test), Y_test).ravel()\n",
    "\n",
    "    measures = dict()\n",
    "    measures[\"acc\"] = (tp + tn)/(tn + fp + fn + tp) * 100\n",
    "    prec = tp/(tp + fp) * 100\n",
    "    recall = tp/(tp + fn) * 100\n",
    "    measures[\"prec\"] = prec\n",
    "    measures[\"recall_sens\"] = recall\n",
    "    measures[\"f1_score\"] = (2 * prec * recall/(prec + recall))\n",
    "    measures['miss_rate'] = (fp + fn) / float(tp + tn + fp + fn) * 100\n",
    "    measures['spec'] = tn/float(tn + fp) * 100\n",
    "    measures['fp_rate'] = fp/float(tn + fp) * 100\n",
    "    allMeasures[i] = measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc</th>\n",
       "      <th>prec</th>\n",
       "      <th>recall_sens</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>miss_rate</th>\n",
       "      <th>spec</th>\n",
       "      <th>fp_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SVC</th>\n",
       "      <td>80.670530</td>\n",
       "      <td>73.211219</td>\n",
       "      <td>73.295129</td>\n",
       "      <td>73.253150</td>\n",
       "      <td>19.329470</td>\n",
       "      <td>84.839650</td>\n",
       "      <td>15.160350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DecisionTreeClassifier</th>\n",
       "      <td>74.751656</td>\n",
       "      <td>67.658844</td>\n",
       "      <td>64.344039</td>\n",
       "      <td>65.959821</td>\n",
       "      <td>25.248344</td>\n",
       "      <td>81.135225</td>\n",
       "      <td>18.864775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MLPClassifier</th>\n",
       "      <td>76.241722</td>\n",
       "      <td>66.742988</td>\n",
       "      <td>67.282170</td>\n",
       "      <td>67.011494</td>\n",
       "      <td>23.758278</td>\n",
       "      <td>81.252017</td>\n",
       "      <td>18.747983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MultinomialNB</th>\n",
       "      <td>81.601821</td>\n",
       "      <td>64.167144</td>\n",
       "      <td>80.997110</td>\n",
       "      <td>71.606515</td>\n",
       "      <td>18.398179</td>\n",
       "      <td>81.844548</td>\n",
       "      <td>18.155452</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              acc       prec  recall_sens   f1_score  \\\n",
       "SVC                     80.670530  73.211219    73.295129  73.253150   \n",
       "DecisionTreeClassifier  74.751656  67.658844    64.344039  65.959821   \n",
       "MLPClassifier           76.241722  66.742988    67.282170  67.011494   \n",
       "MultinomialNB           81.601821  64.167144    80.997110  71.606515   \n",
       "\n",
       "                        miss_rate       spec    fp_rate  \n",
       "SVC                     19.329470  84.839650  15.160350  \n",
       "DecisionTreeClassifier  25.248344  81.135225  18.864775  \n",
       "MLPClassifier           23.758278  81.252017  18.747983  \n",
       "MultinomialNB           18.398179  81.844548  18.155452  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance = pd.DataFrame.from_dict(data=allMeasures, orient='index')\n",
    "performance.set_axis(labels=['SVC', 'DecisionTreeClassifier', 'MLPClassifier', 'MultinomialNB'], inplace=True)\n",
    "# performance.set_axis(labels=['NB'], inplace=True)\n",
    "performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avaliação da performance\n",
    "\n",
    "Iremos carregar as medidas de desempenho dos classificadores treinados na nuvem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = joblib.load(\"scores.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Acc: 80.19942026649659\n",
      "Mean Recall: 75.07368924441352\n",
      "Mean Precision: 73.3399215433968\n",
      "Mean F1: 0.7417334925731642\n",
      "----------------------------------------------------------------\n",
      "Mean Acc: 73.98023089602314\n",
      "Mean Recall: 66.10990348665334\n",
      "Mean Precision: 65.54769388689989\n",
      "Mean F1: 0.6580062981119026\n",
      "----------------------------------------------------------------\n",
      "Mean Acc: 76.26486446269807\n",
      "Mean Recall: 68.96508999217458\n",
      "Mean Precision: 68.58724183634006\n",
      "Mean F1: 0.6874200462937782\n",
      "----------------------------------------------------------------\n",
      "Mean Acc: 80.98464584112138\n",
      "Mean Recall: 64.73748659536851\n",
      "Mean Precision: 81.38778517359695\n",
      "Mean F1: 0.7204236984554557\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for clf in scores:\n",
    "    print(\"Mean Acc: \" + str(np.mean(clf[\"test_accuracy\"])*100))\n",
    "    print(\"Mean Recall: \" + str(np.mean(clf[\"test_recall\"])*100))\n",
    "    print(\"Mean Precision: \" + str(np.mean(clf[\"test_precision\"])*100))\n",
    "    print(\"Mean F1: \" + str(np.mean(clf[\"test_f1\"])))\n",
    "    print(\"----------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.81873727, 0.81059063, 0.81160896, 0.82364934, 0.79918451,\n",
       "       0.80408163, 0.80102041, 0.82755102, 0.79693878, 0.80510204])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
