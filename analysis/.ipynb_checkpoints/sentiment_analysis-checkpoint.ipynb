{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import nltk\n",
    "\n",
    "%matplotlib inline\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BRITISH_AIRLINES = \"British_Airways\"\n",
    "AMERICAN_AIRLINES = \"AmericanAir\"\n",
    "\n",
    "companyUsername = dict()\n",
    "companyUsername['british'] = BRITISH_AIRLINES\n",
    "companyUsername[\"american\"] = AMERICAN_AIRLINES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readCollectedTweets(filename):\n",
    "    df = pd.read_csv(filename, error_bad_lines=False)\n",
    "    df.drop(inplace=True, axis=1, labels=[\"tweet_id\",\n",
    "                                               \"username\",\n",
    "                                              \"permalink\",\n",
    "                                              \"geological_location\"])\n",
    "    return df\n",
    "\n",
    "def getUsersTweets(companiesDataframes, companyUsername):\n",
    "    userDataframes = dict()\n",
    "    \n",
    "    for companyKey in companiesDataframes:\n",
    "        temp_df = companiesDataframes[companyKey]\n",
    "        userDataframes[companyKey] = temp_df[temp_df[\"user_handle\"] != companyUsername[companyKey]]\n",
    "    \n",
    "    return userDataframes\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "def tokenizeTweetSentences(tweetsDict):\n",
    "    \n",
    "    tokenizedSentences = dict()\n",
    "    \n",
    "    for k in tweetsDict:\n",
    "        tokenizedSentences[k] = sent_tokenize(tweetsDict[k])\n",
    "    return tokenizedSentences\n",
    "\n",
    "def tokenizeTweetWords(tokenizedSentences):\n",
    "    tokenizedWords = dict()\n",
    "\n",
    "    for k in tokenizedSentences.keys():\n",
    "        words = list()\n",
    "        \n",
    "        for sentences in tokenizedSentences[k]:\n",
    "            words.append(sentences.split(\" \"))\n",
    "        tokenizedWords[k] = words\n",
    "    \n",
    "    return tokenizedWords\n",
    "\n",
    "def removeStopwords(data, args=None):\n",
    "    \"\"\"\n",
    "        Remove os stops words\n",
    "        \n",
    "        Attributes:\n",
    "            data: String contendo o texto inteiro\n",
    "            EX: data = \"All work and no play makes jack dull boy. \n",
    "            All work and no play makes jack a dull boy.\"\n",
    "        \n",
    "        Return:\n",
    "            Retorna uma lista contendo as palavras do texto filtradas\n",
    "    \n",
    "    \"\"\"\n",
    "    from nltk.corpus import stopwords\n",
    " \n",
    "#     data = \"All work and no play makes jack dull boy. All work and no play makes jack a dull boy.\"\n",
    "    stopWords = list(set(stopwords.words('english')))\n",
    "    \n",
    "    wordsFiltered = []\n",
    "\n",
    "    for w in data:\n",
    "        if w not in stopWords:\n",
    "            wordsFiltered.append(w)\n",
    "\n",
    "    return(wordsFiltered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 15: expected 11 fields, saw 13\\nSkipping line 73: expected 11 fields, saw 13\\nSkipping line 147: expected 11 fields, saw 12\\nSkipping line 193: expected 11 fields, saw 12\\nSkipping line 251: expected 11 fields, saw 12\\nSkipping line 289: expected 11 fields, saw 12\\nSkipping line 397: expected 11 fields, saw 12\\nSkipping line 401: expected 11 fields, saw 12\\nSkipping line 431: expected 11 fields, saw 12\\nSkipping line 452: expected 11 fields, saw 12\\nSkipping line 564: expected 11 fields, saw 12\\nSkipping line 839: expected 11 fields, saw 12\\nSkipping line 1060: expected 11 fields, saw 13\\nSkipping line 1277: expected 11 fields, saw 12\\nSkipping line 1282: expected 11 fields, saw 12\\nSkipping line 1331: expected 11 fields, saw 13\\nSkipping line 1457: expected 11 fields, saw 12\\nSkipping line 1549: expected 11 fields, saw 13\\nSkipping line 1658: expected 11 fields, saw 12\\nSkipping line 1721: expected 11 fields, saw 12\\nSkipping line 1745: expected 11 fields, saw 13\\nSkipping line 2203: expected 11 fields, saw 12\\nSkipping line 2318: expected 11 fields, saw 12\\nSkipping line 2489: expected 11 fields, saw 12\\nSkipping line 2501: expected 11 fields, saw 12\\nSkipping line 2508: expected 11 fields, saw 12\\nSkipping line 2864: expected 11 fields, saw 12\\nSkipping line 2886: expected 11 fields, saw 13\\nSkipping line 3134: expected 11 fields, saw 12\\nSkipping line 3153: expected 11 fields, saw 13\\nSkipping line 3193: expected 11 fields, saw 12\\nSkipping line 3349: expected 11 fields, saw 12\\nSkipping line 3931: expected 11 fields, saw 12\\nSkipping line 3995: expected 11 fields, saw 12\\nSkipping line 4026: expected 11 fields, saw 12\\nSkipping line 4083: expected 11 fields, saw 12\\nSkipping line 4086: expected 11 fields, saw 12\\nSkipping line 4135: expected 11 fields, saw 12\\nSkipping line 4177: expected 11 fields, saw 16\\nSkipping line 4760: expected 11 fields, saw 12\\nSkipping line 4862: expected 11 fields, saw 12\\nSkipping line 4877: expected 11 fields, saw 12\\n'\n"
     ]
    }
   ],
   "source": [
    "british_df = readCollectedTweets(\"british_airlines.csv\")\n",
    "american_df = readCollectedTweets(\"american_airlines.csv\")\n",
    "\n",
    "# Exclui tweets originarios da propria cia\n",
    "companiesDataframes = dict()\n",
    "companiesDataframes[\"british\"] = british_df\n",
    "companiesDataframes[\"american\"] = american_df\n",
    "\n",
    "\n",
    "userDataframes = getUsersTweets(companiesDataframes, companyUsername)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## British Airlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recupera o dataframe da British Airlines\n",
    "britishTweets = userDataframes[\"british\"]\n",
    "\n",
    "britishTokenizedSentences = tokenizeTweetSentences(dict(britishTweets['text']))\n",
    "britishTokenizedWords = tokenizeTweetWords(britishTokenizedSentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "def lexical_diversity(text):\n",
    "    return len(set(text)) / len(text)\n",
    "\n",
    "lexical_diversity(britishTokenizedSentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterWords(tokenized, function, args=None):\n",
    "    filtered = dict()\n",
    "    for k1 in tokenized:\n",
    "        filtered[k1] = dict()\n",
    "        tempList = list()\n",
    "        \n",
    "        # k2 Ã© uma lista de strings\n",
    "        for k2 in tokenized[k1]:\n",
    "            tempList.append(function(k2, args))\n",
    "        filtered[k1] = tempList\n",
    "    return filtered\n",
    "\n",
    "def removeByRegex(listOfWords, regexString):\n",
    "    import re\n",
    "    return [t for t in listOfWords if not re.search(regexString, t)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwordsRemoved = filterWords(britishTokenizedWords, removeStopwords, args=None)\n",
    "callout_regex = \"@[A-Za-z0-9_]+\"\n",
    "withoutCallout = filterWords(britishTokenizedWords, removeByRegex, args=callout_regex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Flight', 'home', 'cancelled!'],\n",
       " ['Apologies',\n",
       "  'to',\n",
       "  'anyone',\n",
       "  'I',\n",
       "  'might',\n",
       "  'have',\n",
       "  'been',\n",
       "  'short',\n",
       "  'with,',\n",
       "  '&',\n",
       "  'some',\n",
       "  'of',\n",
       "  'the',\n",
       "  'problem',\n",
       "  'was',\n",
       "  'out',\n",
       "  'of',\n",
       "  'your',\n",
       "  'hands,',\n",
       "  'but',\n",
       "  'your',\n",
       "  'inability',\n",
       "  'to',\n",
       "  'grip',\n",
       "  'the',\n",
       "  'situation',\n",
       "  '&',\n",
       "  'help',\n",
       "  'clearly',\n",
       "  'frustrated',\n",
       "  'many',\n",
       "  'passengers.'],\n",
       " ['Thanks', '&', 'for', 'helping.']]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "withoutCallout[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "callout_regex = \"@[A-Za-z0-9_]+\"\n",
    "link_regex = 'http[A-Za-z0-9_.-]+'\n",
    "[t for t in tokenizedWords[0][0] if not re.search(callout_regex, t)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## American Airlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "american_df = pd.read_csv(\"american_airlines.csv\")\n",
    "american_df.drop(inplace=True, axis=1, labels=[\"tweet_id\",\n",
    "                                               \"username\",\n",
    "                                              \"permalink\",\n",
    "                                              \"geological_location\"])\n",
    "american_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclui tweets originarios da propria cia\n",
    "americanUsers_df = american_df[american_df[\"user_handle\"] != \"AmericanAir\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "americanText = dict(americanUsers_df[\"text\"])\n",
    "\n",
    "tokenizedSentences = dict()\n",
    "for k in americanText.keys():\n",
    "    tokenizedSentences[k] = sent_tokenize(americanText[k])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizedWords = dict()\n",
    "for k in tokenizedSentences.keys():\n",
    "    words = list()\n",
    "    for sentences in tokenizedSentences[k]:\n",
    "        words.append(sentences.split(\" \"))\n",
    "    tokenizedWords[k] = words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizedWords[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "callout_regex = \"@[A-Za-z0-9_]+\"\n",
    "link_regex = 'http[A-Za-z0-9_.-]+'\n",
    "[t for t in tokenizedWords[0][0] if not re.search(callout_regex, t)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removeStopwords(tokenizedWords[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "snowball = SnowballStemmer('english')\n",
    "wordnet = WordNetLemmatizer()\n",
    "\n",
    "preprocessed_docs = []\n",
    "\n",
    "for doc in removeStopwords(tokenizedWords[0][0]):\n",
    "    final_doc = []\n",
    "    final_doc.append(porter.stem(doc))\n",
    "        #final_doc.append(snowball.stem(word))\n",
    "        #final_doc.append(wordnet.lemmatize(word)) #note that lemmatize() can also takes part of speech as an argument!\n",
    "    preprocessed_docs.append(final_doc)\n",
    "\n",
    "print (preprocessed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
