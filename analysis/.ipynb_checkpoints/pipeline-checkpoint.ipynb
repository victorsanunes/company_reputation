{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetsDF = pd.read_csv(\"preprocessed.csv\")\n",
    "\n",
    "tweetsDF.drop(labels=[\"Unnamed: 0\",\n",
    "                      \"airline\", \n",
    "                      \"negativereason\", \n",
    "                      \"airline_sentiment_confidence\", \n",
    "                      \"negativereason\",\n",
    "                      \"airline_sentiment\",\n",
    "                      \"text\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweet2words</th>\n",
       "      <th>num_capitalized</th>\n",
       "      <th>tweet_length</th>\n",
       "      <th>num_negative_words</th>\n",
       "      <th>num_positive_words</th>\n",
       "      <th>num_neutral_words</th>\n",
       "      <th>has_capitalized</th>\n",
       "      <th>num_capitalised_positive_words</th>\n",
       "      <th>num_capitalised_negative_words</th>\n",
       "      <th>num_hashtags</th>\n",
       "      <th>num_special_character</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13.160358</td>\n",
       "      <td>1</td>\n",
       "      <td>What said</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>plus added commercials experience tacky</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   negativereason_confidence  sentiment  \\\n",
       "0                  13.160358          1   \n",
       "1                   0.000000          1   \n",
       "\n",
       "                               tweet2words  num_capitalized  tweet_length  \\\n",
       "0                                What said                0             3   \n",
       "1  plus added commercials experience tacky                0             6   \n",
       "\n",
       "   num_negative_words  num_positive_words  num_neutral_words  has_capitalized  \\\n",
       "0                   0                   0                  4                1   \n",
       "1                   0                   0                  9                0   \n",
       "\n",
       "   num_capitalised_positive_words  num_capitalised_negative_words  \\\n",
       "0                               0                               0   \n",
       "1                               0                               0   \n",
       "\n",
       "   num_hashtags  num_special_character  \n",
       "0             0                      3  \n",
       "1             0                      4  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweetsDF.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetsDF[\"tweet2words\"] = tweetsDF[\"tweet2words\"].values.astype(\"U\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"sentiment\"\n",
    "features = [c for c in tweetsDF.columns.values if c not in [target]]\n",
    "numeric_features =  [c for c in tweetsDF.columns.values if c not in ['tweet2words', target]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(tweetsDF[features], tweetsDF[target], test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "A criação dos pipelines foi feita utilizando os seguintes kernels no kaggle como auxiliadores;\n",
    "1. [Building A Scikit Learn Classification Pipeline](https://www.kaggle.com/gautham11/building-a-scikit-learn-classification-pipeline)\n",
    "2. [A Deep Dive Into Sklearn Pipelines](https://www.kaggle.com/baghern/a-deep-dive-into-sklearn-pipelines)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transformer to select a single column from the data frame to perform additional transformations on\n",
    "    Use on text columns in the data\n",
    "    \"\"\"\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[self.key]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumberSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transformer to select a single column from the data frame to perform additional transformations on\n",
    "    Use on numeric columns in the data\n",
    "    \"\"\"\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[[self.key]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['negativereason_confidence', 'sentiment', 'tweet2words',\n",
       "       'num_capitalized', 'tweet_length', 'num_negative_words',\n",
       "       'num_positive_words', 'num_neutral_words', 'has_capitalized',\n",
       "       'num_capitalised_positive_words', 'num_capitalised_negative_words',\n",
       "       'num_hashtags', 'num_special_character'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweetsDF.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<9808x10227 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 87497 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = Pipeline([\n",
    "                ('selector', TextSelector(key='tweet2words')),\n",
    "                ('countVec', CountVectorizer(analyzer = \"word\"))\n",
    "            ])\n",
    "\n",
    "text.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "\n",
    "# negConfidence = Pipeline([\n",
    "#                     ('selector', NumberSelector(key='negativereason_confidence')),\n",
    "#                     ('standard', StandardScaler())\n",
    "#                 ])\n",
    "\n",
    "numCapitalized = Pipeline([\n",
    "                    ('selector', NumberSelector(key='num_capitalized')),\n",
    "                    ('standard', StandardScaler())\n",
    "                ])\n",
    "\n",
    "tweetLength = Pipeline([\n",
    "                    ('selector', NumberSelector(key='tweet_length')),\n",
    "                    ('standard', StandardScaler())\n",
    "                ])\n",
    "\n",
    "numNegativeWords = Pipeline([\n",
    "                    ('selector', NumberSelector(key='num_negative_words')),\n",
    "                    ('standard', StandardScaler())\n",
    "                ])\n",
    "\n",
    "numPositiveWords = Pipeline([\n",
    "                    ('selector', NumberSelector(key='num_positive_words')),\n",
    "                    ('standard', StandardScaler())\n",
    "                ])\n",
    "\n",
    "numNeutralWords = Pipeline([\n",
    "                    ('selector', NumberSelector(key='num_neutral_words')),\n",
    "                    ('standard', StandardScaler())\n",
    "                ])\n",
    "\n",
    "\n",
    "numCapitalizedPositiveWords = Pipeline([\n",
    "                    ('selector', NumberSelector(key='num_capitalised_positive_words')),\n",
    "                    ('standard', StandardScaler())\n",
    "                ])\n",
    "\n",
    "numCapitalizedNegativeWords = Pipeline([\n",
    "                    ('selector', NumberSelector(key='num_capitalised_negative_words')),\n",
    "                    ('standard', StandardScaler())\n",
    "                ])\n",
    "\n",
    "numHashtags = Pipeline([\n",
    "                    ('selector', NumberSelector(key='num_hashtags')),\n",
    "                    ('standard', StandardScaler())\n",
    "                ])\n",
    "\n",
    "\n",
    "numSpecialCharacter = Pipeline([\n",
    "                    ('selector', NumberSelector(key='num_special_character')),\n",
    "                    ('standard', StandardScaler())\n",
    "                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<9808x10236 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 156153 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats = FeatureUnion([('text', text),\n",
    "                      ('numCapitalized', numCapitalized),\n",
    "                      (\"tweetLength\", tweetLength),\n",
    "                      (\"numNegativeWords\", numNegativeWords),\n",
    "                      (\"numPositiveWords\", numPositiveWords),\n",
    "                      (\"numNeutralWords\", numNeutralWords),\n",
    "                      (\"numCapitalizedPositiveWords\", numCapitalizedPositiveWords),\n",
    "                      (\"numCapitalizedNegativeWords\", numCapitalizedNegativeWords),\n",
    "                      (\"numHashtags\", numHashtags),\n",
    "                      (\"numSpecialCharacter\", numSpecialCharacter)\n",
    "                     ])\n",
    "\n",
    "feature_processing = Pipeline([('feats', feats)])\n",
    "feature_processing.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# pipeline = Pipeline([\n",
    "#     ('features',feats),\n",
    "#     ('classifier', RandomForestClassifier(n_estimators=200, random_state = 42)),\n",
    "# ])\n",
    "\n",
    "# pipeline.fit(X_train, Y_train)\n",
    "\n",
    "# preds = pipeline.predict(X_test)\n",
    "# np.mean(preds == Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs = list()\n",
    "clfs.append(LogisticRegression())\n",
    "clfs.append(SVC())\n",
    "clfs.append(KNeighborsClassifier(n_neighbors=3))\n",
    "clfs.append(DecisionTreeClassifier())\n",
    "clfs.append(RandomForestClassifier(n_estimators=200, random_state = 42))\n",
    "clfs.append(GradientBoostingClassifier())\n",
    "clfs.append(MLPClassifier())\n",
    "\n",
    "scores = list()\n",
    "pipelineList = list()\n",
    "scores2 = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "-----------------------------------\n",
      "fit_time  mean  0.14806437492370605\n",
      "fit_time  std  0.00904830257954355\n",
      "score_time  mean  0.15582974751790366\n",
      "score_time  std  0.000709279240053915\n",
      "test_accuracy  mean  0.8119900314603967\n",
      "test_accuracy  std  0.0035970302002531753\n",
      "train_accuracy  mean  0.9449939420352488\n",
      "train_accuracy  std  0.0010735654242785579\n",
      "test_recall  mean  0.7440092088787361\n",
      "test_recall  std  0.004413044720313696\n",
      "train_recall  mean  0.9277257608990325\n",
      "train_recall  std  0.0021510150326381878\n",
      "test_precision  mean  0.7559041442220042\n",
      "test_precision  std  0.007729003242404524\n",
      "train_precision  mean  0.9271005124880194\n",
      "train_precision  std  0.00078253041101434\n",
      "test_f1  mean  0.7498760247923465\n",
      "test_f1  std  0.0037413903853414847\n",
      "train_f1  mean  0.9274125195553883\n",
      "train_f1  std  0.0014640503276131828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/victor/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/victor/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/victor/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/victor/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/victor/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/victor/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "-----------------------------------\n",
      "fit_time  mean  6.12412444750468\n",
      "fit_time  std  1.6273857448174296\n",
      "score_time  mean  9.541389226913452\n",
      "score_time  std  0.7998875095835369\n",
      "test_accuracy  mean  0.6212275822455969\n",
      "test_accuracy  std  8.956554920299112e-05\n",
      "train_accuracy  mean  0.6212275725602616\n",
      "train_accuracy  std  4.478962316774625e-05\n",
      "test_recall  mean  0.0\n",
      "test_recall  std  0.0\n",
      "train_recall  mean  0.0\n",
      "train_recall  std  0.0\n",
      "test_precision  mean  0.0\n",
      "test_precision  std  0.0\n",
      "train_precision  mean  0.0\n",
      "train_precision  std  0.0\n",
      "test_f1  mean  0.0\n",
      "test_f1  std  0.0\n",
      "train_f1  mean  0.0\n",
      "train_f1  std  0.0\n",
      "---------------------------------\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=3, p=2,\n",
      "           weights='uniform')\n",
      "-----------------------------------\n",
      "fit_time  mean  0.09487104415893555\n",
      "fit_time  std  0.006024221936189231\n",
      "score_time  mean  4.212922970453898\n",
      "score_time  std  0.47274305488886303\n",
      "test_accuracy  mean  0.7356241828139359\n",
      "test_accuracy  std  0.003085247613878921\n",
      "train_accuracy  mean  0.8591457038568802\n",
      "train_accuracy  std  0.0015245081660649705\n",
      "test_recall  mean  0.7782000179935614\n",
      "test_recall  std  0.010480398596163735\n",
      "train_recall  mean  0.8788684111379892\n",
      "train_recall  std  0.01411442875408953\n",
      "test_precision  mean  0.6204475409617612\n",
      "test_precision  std  0.004500688063692692\n",
      "train_precision  mean  0.7782312894382944\n",
      "train_precision  std  0.007903667841366707\n",
      "test_f1  mean  0.6903719726389369\n",
      "test_f1  std  0.003748333149490661\n",
      "train_f1  mean  0.8253570069225122\n",
      "train_f1  std  0.002472209679462918\n",
      "---------------------------------\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "-----------------------------------\n",
      "fit_time  mean  0.09730339050292969\n",
      "fit_time  std  0.0005379756967355177\n",
      "score_time  mean  0.16044179598490396\n",
      "score_time  std  0.008761437411965915\n",
      "test_accuracy  mean  1.0\n",
      "test_accuracy  std  0.0\n",
      "train_accuracy  mean  1.0\n",
      "train_accuracy  std  0.0\n",
      "test_recall  mean  1.0\n",
      "test_recall  std  0.0\n",
      "train_recall  mean  1.0\n",
      "train_recall  std  0.0\n",
      "test_precision  mean  1.0\n",
      "test_precision  std  0.0\n",
      "train_precision  mean  1.0\n",
      "train_precision  std  0.0\n",
      "test_f1  mean  1.0\n",
      "test_f1  std  0.0\n",
      "train_f1  mean  1.0\n",
      "train_f1  std  0.0\n",
      "---------------------------------\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=1,\n",
      "            oob_score=False, random_state=42, verbose=0, warm_start=False)\n",
      "-----------------------------------\n",
      "fit_time  mean  6.887546857198079\n",
      "fit_time  std  0.26535595301102977\n",
      "score_time  mean  1.1345791816711426\n",
      "score_time  std  0.021603374287822\n",
      "test_accuracy  mean  0.9634988301746645\n",
      "test_accuracy  std  0.003166344831579502\n",
      "train_accuracy  mean  1.0\n",
      "train_accuracy  std  0.0\n",
      "test_recall  mean  0.921128874320189\n",
      "test_recall  std  0.008208205286289851\n",
      "train_recall  mean  1.0\n",
      "train_recall  std  0.0\n",
      "test_precision  mean  0.9813672721231445\n",
      "test_precision  std  0.0027616181387978478\n",
      "train_precision  mean  1.0\n",
      "train_precision  std  0.0\n",
      "test_f1  mean  0.9502731461245763\n",
      "test_f1  std  0.004520932709342937\n",
      "train_f1  mean  1.0\n",
      "train_f1  std  0.0\n",
      "---------------------------------\n",
      "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "              max_features=None, max_leaf_nodes=None,\n",
      "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "              min_samples_leaf=1, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "-----------------------------------\n",
      "fit_time  mean  0.773241917292277\n",
      "fit_time  std  0.010992615832198361\n",
      "score_time  mean  0.16422454516092935\n",
      "score_time  std  0.0023132699832210724\n",
      "test_accuracy  mean  1.0\n",
      "test_accuracy  std  0.0\n",
      "train_accuracy  mean  1.0\n",
      "train_accuracy  std  0.0\n",
      "test_recall  mean  1.0\n",
      "test_recall  std  0.0\n",
      "train_recall  mean  1.0\n",
      "train_recall  std  0.0\n",
      "test_precision  mean  1.0\n",
      "test_precision  std  0.0\n",
      "train_precision  mean  1.0\n",
      "train_precision  std  0.0\n",
      "test_f1  mean  1.0\n",
      "test_f1  std  0.0\n",
      "train_f1  mean  1.0\n",
      "train_f1  std  0.0\n",
      "---------------------------------\n",
      "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
      "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
      "       verbose=False, warm_start=False)\n",
      "-----------------------------------\n",
      "fit_time  mean  60.558612744013466\n",
      "fit_time  std  1.0084410674539508\n",
      "score_time  mean  0.17644445101420084\n",
      "score_time  std  0.0008606063173480646\n",
      "test_accuracy  mean  0.7966967986731066\n",
      "test_accuracy  std  0.0029430479783158813\n",
      "train_accuracy  mean  0.9986745409838543\n",
      "train_accuracy  std  0.0001908188193907226\n",
      "test_recall  mean  0.7117118526718483\n",
      "test_recall  std  0.006868469094532343\n",
      "train_recall  mean  0.9983848172166159\n",
      "train_recall  std  0.0006596619115034962\n",
      "test_precision  mean  0.7412966938575933\n",
      "test_precision  std  0.005719726343584074\n",
      "train_precision  mean  0.9981164332793336\n",
      "train_precision  std  0.00018862320899902294\n",
      "test_f1  mean  0.7261688338220079\n",
      "test_f1  std  0.0039655939027185595\n",
      "train_f1  mean  0.9982504354711593\n",
      "train_f1  std  0.0002526901143224082\n"
     ]
    }
   ],
   "source": [
    "for c in clfs:\n",
    "    pipeline.set_params(classifier = c)\n",
    "    pipelineList.append(pipeline)\n",
    "    s = cross_validate(pipeline, X_train, Y_train, scoring=[\"accuracy\", \"recall\", \"precision\", \"f1\"])\n",
    "    scores.append(s)\n",
    "    pipeline.fit(X_train, Y_train)\n",
    "    scores2.append(pipeline.score(X_train, Y_train))\n",
    "    print('---------------------------------')\n",
    "    print(str(c))\n",
    "    print('-----------------------------------')\n",
    "    for key, values in s.items():\n",
    "            print(key,' mean ', values.mean())\n",
    "            print(key,' std ', values.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pickle.dumps(pipelineList[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(pipelineList)):\n",
    "    joblib.dump(pipelineList[i], \"pipeline[\" +str(i)+ \"].joblib\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
