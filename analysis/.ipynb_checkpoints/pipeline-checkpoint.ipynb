{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetsDF = pd.read_csv(\"preprocessed.csv\")\n",
    "\n",
    "tweetsDF.drop(labels=[\"Unnamed: 0\",\n",
    "                      \"airline\", \n",
    "                      \"negativereason\", \n",
    "                      \"airline_sentiment_confidence\", \n",
    "                      \"negativereason\",\n",
    "                      \"airline_sentiment\",\n",
    "                      \"text\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweet2words</th>\n",
       "      <th>num_capitalized</th>\n",
       "      <th>tweet_length</th>\n",
       "      <th>num_negative_words</th>\n",
       "      <th>num_positive_words</th>\n",
       "      <th>num_neutral_words</th>\n",
       "      <th>has_capitalized</th>\n",
       "      <th>num_capitalised_positive_words</th>\n",
       "      <th>num_capitalised_negative_words</th>\n",
       "      <th>num_hashtags</th>\n",
       "      <th>num_special_character</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13.160358</td>\n",
       "      <td>1</td>\n",
       "      <td>What said</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>plus added commercials experience tacky</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   negativereason_confidence  sentiment  \\\n",
       "0                  13.160358          1   \n",
       "1                   0.000000          1   \n",
       "\n",
       "                               tweet2words  num_capitalized  tweet_length  \\\n",
       "0                                What said                0             3   \n",
       "1  plus added commercials experience tacky                0             6   \n",
       "\n",
       "   num_negative_words  num_positive_words  num_neutral_words  has_capitalized  \\\n",
       "0                   0                   0                  4                1   \n",
       "1                   0                   0                  9                0   \n",
       "\n",
       "   num_capitalised_positive_words  num_capitalised_negative_words  \\\n",
       "0                               0                               0   \n",
       "1                               0                               0   \n",
       "\n",
       "   num_hashtags  num_special_character  \n",
       "0             0                      3  \n",
       "1             0                      4  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweetsDF.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetsDF[\"tweet2words\"] = tweetsDF[\"tweet2words\"].values.astype(\"U\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"sentiment\"\n",
    "features = [c for c in tweetsDF.columns.values if c not in [target]]\n",
    "numeric_features =  [c for c in tweetsDF.columns.values if c not in ['tweet2words', target]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(tweetsDF[features], tweetsDF[target], test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transformer to select a single column from the data frame to perform additional transformations on\n",
    "    Use on text columns in the data\n",
    "    \"\"\"\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[self.key]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumberSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transformer to select a single column from the data frame to perform additional transformations on\n",
    "    Use on numeric columns in the data\n",
    "    \"\"\"\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[[self.key]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<9808x10227 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 87497 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = Pipeline([\n",
    "                ('selector', TextSelector(key='tweet2words')),\n",
    "                ('countVec', CountVectorizer(analyzer = \"word\"))\n",
    "            ])\n",
    "\n",
    "text.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00158819],\n",
       "       [-0.17092916],\n",
       "       [-0.17537903],\n",
       "       ...,\n",
       "       [-0.16641288],\n",
       "       [-0.16641288],\n",
       "       [-0.00158819]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "length =  Pipeline([\n",
    "                ('selector', NumberSelector(key='negativereason_confidence')),\n",
    "                ('standard', StandardScaler())\n",
    "            ])\n",
    "\n",
    "length.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<9808x10227 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 87497 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats = FeatureUnion([('text', text)])\n",
    "\n",
    "feature_processing = Pipeline([('feats', feats)])\n",
    "feature_processing.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8116721854304636"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features',feats),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=200, random_state = 42)),\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, Y_train)\n",
    "\n",
    "preds = pipeline.predict(X_test)\n",
    "np.mean(preds == Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs = list()\n",
    "clfs.append(LogisticRegression())\n",
    "clfs.append(SVC())\n",
    "clfs.append(KNeighborsClassifier(n_neighbors=3))\n",
    "clfs.append(DecisionTreeClassifier())\n",
    "clfs.append(RandomForestClassifier())\n",
    "clfs.append(GradientBoostingClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "-----------------------------------\n",
      "fit_time  mean  0.2993203004201253\n",
      "fit_time  std  0.24355048146692487\n",
      "score_time  mean  0.03563459714253744\n",
      "score_time  std  0.0005173868599853422\n",
      "test_score  mean  0.8068918506377988\n",
      "test_score  std  0.0054046896093935416\n",
      "train_score  mean  0.9445351173036453\n",
      "train_score  std  0.0006120316421246042\n",
      "---------------------------------\n",
      "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "-----------------------------------\n",
      "fit_time  mean  4.169328689575195\n",
      "fit_time  std  0.01716559278210503\n",
      "score_time  mean  1.6866902510325115\n",
      "score_time  std  0.009704038962201044\n",
      "test_score  mean  0.6212275822455969\n",
      "test_score  std  8.956554920299112e-05\n",
      "train_score  mean  0.6212275725602616\n",
      "train_score  std  4.478962316774625e-05\n",
      "---------------------------------\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=3, p=2,\n",
      "           weights='uniform')\n",
      "-----------------------------------\n",
      "fit_time  mean  0.07951219876607259\n",
      "fit_time  std  0.0015526289103503642\n",
      "score_time  mean  0.5313748518625895\n",
      "score_time  std  0.010038056208846588\n",
      "test_score  mean  0.5886007591781318\n",
      "test_score  std  0.005437564241247943\n",
      "train_score  mean  0.7832898756990182\n",
      "train_score  std  0.010427726243633242\n",
      "---------------------------------\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "-----------------------------------\n",
      "fit_time  mean  0.7633225917816162\n",
      "fit_time  std  0.01211001717426309\n",
      "score_time  mean  0.03663778305053711\n",
      "score_time  std  0.0005387400386980092\n",
      "test_score  mean  0.733890228193118\n",
      "test_score  std  0.005466158041706108\n",
      "train_score  mean  0.9977059621079869\n",
      "train_score  std  0.00012473199934971477\n",
      "---------------------------------\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "-----------------------------------\n",
      "fit_time  mean  0.6642776330312093\n",
      "fit_time  std  0.006627331229444508\n",
      "score_time  mean  0.05450256665547689\n",
      "score_time  std  0.0004370380269941645\n",
      "test_score  mean  0.7765077930667384\n",
      "test_score  std  0.008186692496966765\n",
      "train_score  mean  0.9857260029098377\n",
      "train_score  std  0.0018018121837549965\n",
      "---------------------------------\n",
      "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "              max_features=None, max_leaf_nodes=None,\n",
      "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "              min_samples_leaf=1, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "              warm_start=False)\n",
      "-----------------------------------\n",
      "fit_time  mean  1.142585039138794\n",
      "fit_time  std  0.002402585208810197\n",
      "score_time  mean  0.03881112734476725\n",
      "score_time  std  0.00041340673992313633\n",
      "test_score  mean  0.7422504801382274\n",
      "test_score  std  0.005485167210657887\n",
      "train_score  mean  0.7538742258390109\n",
      "train_score  std  0.0027056015248995394\n"
     ]
    }
   ],
   "source": [
    "for c in clfs:\n",
    "    pipeline.set_params(classifier = c)\n",
    "    scores = cross_validate(pipeline, X_train, Y_train)\n",
    "    print('---------------------------------')\n",
    "    print(str(c))\n",
    "    print('-----------------------------------')\n",
    "    for key, values in scores.items():\n",
    "            print(key,' mean ', values.mean())\n",
    "            print(key,' std ', values.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
