{"cells":[{"metadata":{"_cell_guid":"0eac874b-1bea-4cf4-952c-e302f5c44ea2","_uuid":"6e0a34007c208412207de2bbebbdec24ed540a7b"},"cell_type":"markdown","source":"# Introduction\nIn my previous kernel, I explored [deep learning with Keras](https://www.kaggle.com/bertcarremans/deep-learning-for-sentiment-analysis) on the [TwitterAirline data set](https://www.kaggle.com/crowdflower/twitter-airline-sentiment). The model with Dropout layers produced the best results. However, it was still outperformed by the LogisticRegression model. Let's try if we can do better than the LogisticRegression with a deep learning model that uses word embeddings. \n\nIn this kernel we will:\n* use the Embedding layer of Keras to create word embeddings from the training data\n* use pretrained word embeddings trained on a much larger corpus"},{"metadata":{"_cell_guid":"af6fd7d5-043e-43ab-acf6-bb438b779417","_uuid":"06440c19546700dd01d9d649ddc735e03206553c"},"cell_type":"markdown","source":"# Word Embeddings\nWhen applying one-hot encoding to the words in the tweets, we end up with sparse vectors of high dimensionality (here the number of words). On larger data sets this could cause performance issues. Additionally, one-hot encoding does not take into account the semantics of the words. For instance, *plane* and *aircraft* are different words but have a similar meaning. \n\nWord embeddings reduce these two issues. Word embeddings are dense vectors with a much lower dimensionality. Secondly, the semantic relationships between words are reflected in the distance and direction of the vectors.  "},{"metadata":{"_cell_guid":"2afb7748-1536-41f7-a2ad-3dfbdee61263","_uuid":"655d234070fef5815ed4d9618f4e430508b8809d"},"cell_type":"markdown","source":"# Set-up of the project"},{"metadata":{"_cell_guid":"21cee267-f257-4ba3-88e1-d22e2fc57c7e","_uuid":"2fec7805d8d0e999fc405d136beff46ac331b963","trusted":false,"collapsed":true},"cell_type":"code","source":"# Basic packages\nimport pandas as pd \nimport numpy as np\nimport re\nimport collections\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\n\n# Packages for data preparation\nfrom sklearn.model_selection import train_test_split\nfrom nltk.corpus import stopwords\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils.np_utils import to_categorical\nfrom sklearn.preprocessing import LabelEncoder\n\n# Packages for modeling\nfrom keras import models\nfrom keras import layers\nfrom keras import regularizers","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1eeb591e-330a-4177-be9a-466b388cc2a1","_uuid":"6f2221dcf02f8bb28bf295ed95f6cf41c715dab4","collapsed":true,"trusted":false},"cell_type":"code","source":"NB_WORDS = 10000  # Parameter indicating the number of words we'll put in the dictionary\nVAL_SIZE = 1000  # Size of the validation set\nNB_START_EPOCHS = 20  # Number of epochs we usually start to train with\nBATCH_SIZE = 512  # Size of the batches used in the mini-batch gradient descent\nMAX_LEN = 24  # Maximum number of words in a sequence\nGLOVE_DIM = 50  # Number of dimensions of the GloVe word embeddings\nINPUT_PATH = '../input'  # Path where all input files are stored","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8c2752c3-bd49-445f-accb-a5b4dd45513c","_uuid":"b76c5c715098835b2e07a83846f5691188d34c01","collapsed":true,"trusted":false},"cell_type":"code","source":"root = Path('../')\ninput_path = root / 'input/' \nouput_path = root / 'output/'\nsource_path = root / 'source/'","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3b631288-d13d-4b59-a7b2-478a1db1622d","_uuid":"d7c067163d94f765212d7979bb657bdac67846aa"},"cell_type":"markdown","source":"# Some helper functions"},{"metadata":{"_cell_guid":"e81467bb-4fc4-42eb-bf2d-733f28ebbfe2","_uuid":"65806a53f6e279fd16c12b986c2fe9d9158de57b","collapsed":true,"trusted":false},"cell_type":"code","source":"def deep_model(model, X_train, y_train, X_valid, y_valid):\n    '''\n    Function to train a multi-class model. The number of epochs and \n    batch_size are set by the constants at the top of the\n    notebook. \n    \n    Parameters:\n        model : model with the chosen architecture\n        X_train : training features\n        y_train : training target\n        X_valid : validation features\n        Y_valid : validation target\n    Output:\n        model training history\n    '''\n    model.compile(optimizer='rmsprop'\n                  , loss='categorical_crossentropy'\n                  , metrics=['accuracy'])\n    \n    history = model.fit(X_train\n                       , y_train\n                       , epochs=NB_START_EPOCHS\n                       , batch_size=BATCH_SIZE\n                       , validation_data=(X_valid, y_valid)\n                       , verbose=1)\n    return history\n\n\ndef eval_metric(history, metric_name):\n    '''\n    Function to evaluate a trained model on a chosen metric. \n    Training and validation metric are plotted in a\n    line chart for each epoch.\n    \n    Parameters:\n        history : model training history\n        metric_name : loss or accuracy\n    Output:\n        line chart with epochs of x-axis and metric on\n        y-axis\n    '''\n    metric = history.history[metric_name]\n    val_metric = history.history['val_' + metric_name]\n\n    e = range(1, NB_START_EPOCHS + 1)\n\n    plt.plot(e, metric, 'bo', label='Train ' + metric_name)\n    plt.plot(e, val_metric, 'b', label='Validation ' + metric_name)\n    plt.legend()\n    plt.show()\n\ndef test_model(model, X_train, y_train, X_test, y_test, epoch_stop):\n    '''\n    Function to test the model on new data after training it\n    on the full training data with the optimal number of epochs.\n    \n    Parameters:\n        model : trained model\n        X_train : training features\n        y_train : training target\n        X_test : test features\n        y_test : test target\n        epochs : optimal number of epochs\n    Output:\n        test accuracy and test loss\n    '''\n    model.fit(X_train\n              , y_train\n              , epochs=epoch_stop\n              , batch_size=BATCH_SIZE\n              , verbose=0)\n    results = model.evaluate(X_test, y_test)\n    \n    return results\n\ndef remove_stopwords(input_text):\n    '''\n    Function to remove English stopwords from a Pandas Series.\n    \n    Parameters:\n        input_text : text to clean\n    Output:\n        cleaned Pandas Series \n    '''\n    stopwords_list = stopwords.words('english')\n    # Some words which might indicate a certain sentiment are kept via a whitelist\n    whitelist = [\"n't\", \"not\", \"no\"]\n    words = input_text.split() \n    clean_words = [word for word in words if (word not in stopwords_list or word in whitelist) and len(word) > 1] \n    return \" \".join(clean_words) \n    \ndef remove_mentions(input_text):\n    '''\n    Function to remove mentions, preceded by @, in a Pandas Series\n    \n    Parameters:\n        input_text : text to clean\n    Output:\n        cleaned Pandas Series \n    '''\n    return re.sub(r'@\\w+', '', input_text)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"30b2a5cd-6e5c-4850-9fc3-acbaa34d8640","_uuid":"f6eff68f92d117c2556e403bc97c7800521ce4eb"},"cell_type":"markdown","source":"# Data Preparation\n### Reading and cleaning data"},{"metadata":{"_cell_guid":"ad9e0531-8a58-4c5b-974d-891745af8ac5","_uuid":"eb600f7b6b278e73117bfea5e6b297fb10aa2d56","collapsed":true,"trusted":false},"cell_type":"code","source":"tweets_dir = \"twitter-airline-sentiment/\"\ndf = pd.read_csv(input_path / tweets_dir / 'Tweets.csv')\ndf = df.reindex(np.random.permutation(df.index))  \ndf = df[['text', 'airline_sentiment']]\ndf.text = df.text.apply(remove_stopwords).apply(remove_mentions)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"62132bff-f75d-4d9e-b4d5-d9c1ced79e3f","_uuid":"fa8df3743c04dfda0ec297245bc102d9b3227b03"},"cell_type":"markdown","source":"### Train-Test split"},{"metadata":{"_cell_guid":"df742ade-52d3-4b47-80b0-1f7cc26545eb","_uuid":"6ab9296159749e4525244fae6d6f4e6a37121c93","trusted":false,"collapsed":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(df.text, df.airline_sentiment, test_size=0.1, random_state=37)\nprint('# Train data samples:', X_train.shape[0])\nprint('# Test data samples:', X_test.shape[0])\nassert X_train.shape[0] == y_train.shape[0]\nassert X_test.shape[0] == y_test.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b76321cb-6b78-49e5-9f81-72b9ce0c0fff","_uuid":"083757435330306da9b9fa9aae610ba76e20471e"},"cell_type":"markdown","source":"### Converting words to numbers"},{"metadata":{"_cell_guid":"77a831f6-0261-478e-94ec-72734e0373e4","_uuid":"b7dd99fd85fab14bf865f7d984766911f048cdeb","collapsed":true,"trusted":false},"cell_type":"code","source":"tk = Tokenizer(num_words=NB_WORDS,\n               filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n               lower=True,\n               split=\" \")\ntk.fit_on_texts(X_train)\n\nX_train_seq = tk.texts_to_sequences(X_train)\nX_test_seq = tk.texts_to_sequences(X_test)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"53358459-0564-4f74-a8d3-53fbb9070260","_uuid":"554f40d8dd9f7f37147731b97d5581da6ad7112d"},"cell_type":"markdown","source":"### Creating word sequences of equal length\nBefore we can compute the word embeddings, we need to make sure the sequences are of equal length. In the example below, we truncate sequences to length MAX_LEN, or pad them with zeroes to achieve this. First, we'll have a look at the length of the (cleaned) tweets."},{"metadata":{"_cell_guid":"1c65a612-b310-4e94-bb90-e98a5543bc1b","_uuid":"ec28cfcc5c7d88d2ac2da645ebd331c4f983d3d6","trusted":false,"collapsed":true},"cell_type":"code","source":"seq_lengths = X_train.apply(lambda x: len(x.split(' ')))\nseq_lengths.describe()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7fbdae06-3e2d-4e58-beec-b526c4ceb49d","_uuid":"0801bf3d7e2adcc13d07855f052578038d24ba34"},"cell_type":"markdown","source":"Based on the figures above we will set MAX_LEN to 24. So this means we will not be truncating any words, only pad with zeros. This is to avoid to lose information as the tweets are rather short."},{"metadata":{"_cell_guid":"4caee2ba-a258-4908-b58c-c6280a35e436","_uuid":"dde2572f8b73b2ba795e4a5a9a6333b06924990d","collapsed":true,"trusted":false},"cell_type":"code","source":"X_train_seq_trunc = pad_sequences(X_train_seq, maxlen=MAX_LEN)\nX_test_seq_trunc = pad_sequences(X_test_seq, maxlen=MAX_LEN)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3d2bf008-932b-4b44-bea4-52626876d419","_uuid":"363f55f2218dc983ea5f7aeb605321becbfeb300","trusted":false,"collapsed":true},"cell_type":"code","source":"X_train_seq_trunc[10]  # Example of padded sequence","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b0188904-9adc-4b01-823b-24a6281b4dc9","_uuid":"afa786f39cdc5a09bb40ffb4a65ea02ff917575c"},"cell_type":"markdown","source":"### Converting the target classes to numbers"},{"metadata":{"_cell_guid":"e8cde24c-641b-474f-a569-fa1a778dec6c","_uuid":"6b2b84b137f44315eade504c8d4675a50bf78bc6","collapsed":true,"trusted":false},"cell_type":"code","source":"le = LabelEncoder()\ny_train_le = le.fit_transform(y_train)\ny_test_le = le.transform(y_test)\ny_train_oh = to_categorical(y_train_le)\ny_test_oh = to_categorical(y_test_le)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"33c4e935-4d09-45d0-a0db-8e79e24aee09","_uuid":"59cd92aa3bc7417176ce2d8dc1a1907823cb1d83"},"cell_type":"markdown","source":"### Splitting off validation data"},{"metadata":{"_cell_guid":"1ff1d8d2-2d98-4449-9298-e2594bed577d","_uuid":"4cf52b83651a836882276c58b3c5000ba430b443","trusted":false,"collapsed":true},"cell_type":"code","source":"X_train_emb, X_valid_emb, y_train_emb, y_valid_emb = train_test_split(X_train_seq_trunc, y_train_oh, test_size=0.1, random_state=37)\n\nassert X_valid_emb.shape[0] == y_valid_emb.shape[0]\nassert X_train_emb.shape[0] == y_train_emb.shape[0]\n\nprint('Shape of validation set:',X_valid_emb.shape)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8101462d-eba1-4fae-9e77-a2a1166aa2c3","_uuid":"c6f2dcdeebbef27add78e0eb54ac76ce19052d96"},"cell_type":"markdown","source":"# Modeling"},{"metadata":{"_cell_guid":"706bdf6e-7248-48e5-9740-3bd16b3d8dfc","_uuid":"0f9fe95cca79087a8cc87396cc62420fe4e41043"},"cell_type":"markdown","source":"### Training word embeddings\nKeras provides an **Embedding layer** which helps us to train specific word embeddings based on our training data. It will convert the words in our vocabulary to multi-dimensional vectors. "},{"metadata":{"_cell_guid":"be7731ff-5fd4-4006-93ab-7935608870a8","_uuid":"9b0f7a95f5ce87f2f63dac719eb47cced4ed5d4b","trusted":false,"collapsed":true},"cell_type":"code","source":"emb_model = models.Sequential()\nemb_model.add(layers.Embedding(NB_WORDS, 8, input_length=MAX_LEN))\nemb_model.add(layers.Flatten())\nemb_model.add(layers.Dense(3, activation='softmax'))\nemb_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"97113b34-2ee1-4edf-a411-5cfd6c2033e0","_uuid":"1227cf605366f4be25b74dcba1a29389d11ab680","trusted":false,"collapsed":true},"cell_type":"code","source":"emb_history = deep_model(emb_model, X_train_emb, y_train_emb, X_valid_emb, y_valid_emb)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"bfe2670e-9d2e-49f1-a260-45c201e17a92","_uuid":"976697d3ff94e68ccbcef79011598b519230e22c"},"cell_type":"markdown","source":"We have a validation accuracy of about 77%. The number of words in the tweets is rather low, so this result is rather good. \n\nBy comparing the training and validation accuracy and loss, we see that the model starts overfitting from epoch 6.."},{"metadata":{"_cell_guid":"97389a42-d903-4ade-bf58-8a0feb6d84fa","_uuid":"f34fd856c0b4b19c9630e372bcb739ee4c24c2c1","trusted":false,"collapsed":true},"cell_type":"code","source":"eval_metric(emb_history, 'acc')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"24dc6adb-7368-44c4-836b-5f7a88b89c9c","_uuid":"825b7a981d45c79419f9768d5b30b3a4483380e3","trusted":false,"collapsed":true},"cell_type":"code","source":"eval_metric(emb_history, 'loss')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b2ee9d54-d504-4cba-9738-fa8acb2af538","_uuid":"41d0e88fa1528fd93253b4a1bdeb3be3b3ba504d","trusted":false,"collapsed":true},"cell_type":"code","source":"emb_results = test_model(emb_model, X_train_seq_trunc, y_train_oh, X_test_seq_trunc, y_test_oh, 6)\nprint('/n')\nprint('Test accuracy of word embeddings model: {0:.2f}%'.format(emb_results[1]*100))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5d7aa101-acce-4a75-a88b-b7e1c49a32ca","_uuid":"45ef94d2bb38d78ddf288d15237e51aca8dad600"},"cell_type":"markdown","source":"This test result is quite ok, but still not better than the other deep learning model with Dropout layers, nor the LogisticRegression. Let's see if we can improve with pretrained word embeddings.\n\n### Using pre-trained word embeddings\nBecause the training data is not so big, the model might not be able to learn good embeddings for the sentiment analysis. Luckily we can load pre-trained word embeddings built on a much larger training data. \n\nThe [GloVe database](https://nlp.stanford.edu/projects/glove/) contains multiple pre-trained word embeddings, and more specific embeddings trained on tweets."},{"metadata":{"_cell_guid":"189abce5-0663-4bc1-9e33-66a86f6052b1","_uuid":"13f3c179266bb21577200f7ff62f4d3debf07449","collapsed":true,"trusted":false},"cell_type":"code","source":"glove_file = 'glove.twitter.27B.' + str(GLOVE_DIM) + 'd.txt'\nglove_dir = 'glove-global-vectors-for-word-representation/'\nemb_dict = {}\nglove = open(input_path / glove_dir / glove_file)\nfor line in glove:\n    values = line.split()\n    word = values[0]\n    vector = np.asarray(values[1:], dtype='float32')\n    emb_dict[word] = vector\nglove.close()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3d2183ad-84da-455b-9442-e84a38a34461","_uuid":"ec086a128a08cb835e425adc6338f85da32325ce"},"cell_type":"markdown","source":"Let's test if we can find some airline related words in the dictionary."},{"metadata":{"_cell_guid":"c972f699-652c-493f-ac18-fd091b3f9289","_uuid":"5957cc228233c9b96f0ad93e0632d91473cd97de","trusted":false,"collapsed":true},"cell_type":"code","source":"airline_words = ['airplane', 'airline', 'flight', 'luggage']\nfor w in airline_words:\n    if w in emb_dict.keys():\n        print('Found the word {} in the dictionary'.format(w))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"17b50696-d448-483b-a4ac-a7039ea8f251","_uuid":"a5638dd38ef160a7632f7dbc7e47ef055654a07f"},"cell_type":"markdown","source":"To feed this into an Embedding layer, we need to build a matrix containing the words in the tweets and their representative word embedding. So this matrix will be of shape (NB_WORDS, GLOVE_DIM)"},{"metadata":{"_cell_guid":"bdd5d3b7-f19c-448a-a7bc-defcfe8ea034","_uuid":"68cb90dd9939915adc4c1500e46ca76bfeb485bc","collapsed":true,"trusted":false},"cell_type":"code","source":"emb_matrix = np.zeros((NB_WORDS, GLOVE_DIM))\n\nfor w, i in tk.word_index.items():\n    # The word_index contains a token for all words of the training data so we need to limit that\n    if i < NB_WORDS:\n        vect = emb_dict.get(w)\n        # Check if the word from the training data occurs in the GloVe word embeddings\n        # Otherwise the vector is kept with only zeros\n        if vect is not None:\n            emb_matrix[i] = vect\n    else:\n        break","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f2edc95b-5654-4c04-97a9-57fd2e4a56a0","_uuid":"4728c6ccb4308ed6f2efffe0545e564acc13bcba","trusted":false,"collapsed":true},"cell_type":"code","source":"glove_model = models.Sequential()\nglove_model.add(layers.Embedding(NB_WORDS, GLOVE_DIM, input_length=MAX_LEN))\nglove_model.add(layers.Flatten())\nglove_model.add(layers.Dense(3, activation='softmax'))\nglove_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b21eae7c-c9b4-495d-ba20-11ccfeedcb64","_uuid":"06376000aac2e4904fd0591f27f445cf9a103026"},"cell_type":"markdown","source":"With the *set_weights* method we load the pre-trained embeddings in the Embedding layer (here layer 0). By setting the *trainable* attribute to False, we make sure not to change the pre-trained embeddings."},{"metadata":{"_cell_guid":"75967b04-88f6-4785-9f97-a5a8f1a99a85","_uuid":"8487f958f0fc57706c9b649f839fbec41c6032a6","collapsed":true,"trusted":false},"cell_type":"code","source":"glove_model.layers[0].set_weights([emb_matrix])\nglove_model.layers[0].trainable = False","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c2301196-b995-4e71-8b34-21367b4f8962","_uuid":"8febdbc6b2e9598e06228b0b4b492552d7c5ec9f","trusted":false,"collapsed":true},"cell_type":"code","source":"glove_history = deep_model(glove_model, X_train_emb, y_train_emb, X_valid_emb, y_valid_emb)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"17c7fd72-2e4d-49e3-bbf2-cb7ad98bdafd","_uuid":"a87cfcffc7ce7a57fbe0fe961608d0880d884913","trusted":false,"collapsed":true},"cell_type":"code","source":"eval_metric(glove_history, 'loss')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"df249c7f-bd7d-4b54-8e2f-3328518c4db9","_uuid":"668daafacb1a99527158ce11df268144cd2877ae","trusted":false,"collapsed":true},"cell_type":"code","source":"eval_metric(glove_history, 'acc')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8bf5090f-9fd3-477e-92a3-c38445b9f735","_uuid":"b2d7e0010319448b7a47b45599c42f923c79b418","trusted":false,"collapsed":true},"cell_type":"code","source":"glove_results = test_model(glove_model, X_train_seq_trunc, y_train_oh, X_test_seq_trunc, y_test_oh, 3)\nprint('/n')\nprint('Test accuracy of word glove model: {0:.2f}%'.format(glove_results[1]*100))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e1c3cf1e-529c-40f3-bf47-16f13338ce8d","_uuid":"1ff4afd68e9ea50de43d22cd64e59dbba4a09409"},"cell_type":"markdown","source":"The model overfits fast, after 3 epochs. Moreover the validation accuracy is lower compared to the embeddings trained on the training data. \n\nAs a final exercise, let's see what results we get when we train the embeddings with the same number of dimensions as the GloVe data.\n\n### Training word embeddings with more dimensions"},{"metadata":{"_cell_guid":"d6b74e20-1320-4095-889d-c79489567a7e","_uuid":"2f84b6e5a84ad918f1a7bef9eaf5c36a68748b99","trusted":false,"collapsed":true},"cell_type":"code","source":"emb_model2 = models.Sequential()\nemb_model2.add(layers.Embedding(NB_WORDS, GLOVE_DIM, input_length=MAX_LEN))\nemb_model2.add(layers.Flatten())\nemb_model2.add(layers.Dense(3, activation='softmax'))\nemb_model2.summary()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4a5ddb8d-84d8-46cd-8679-76ee052b81a5","_uuid":"6856e89e9bb3e295b5f2319200fe1b18e1d5b02b","trusted":false,"collapsed":true},"cell_type":"code","source":"emb_history2 = deep_model(emb_model2, X_train_emb, y_train_emb, X_valid_emb, y_valid_emb)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f983431c-96bb-4857-ad43-8dd9b2a614cc","_uuid":"d48a27e29d1997e60636a87c40028267b52200cf","trusted":false,"collapsed":true},"cell_type":"code","source":"eval_metric(glove_history, 'loss')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1b6c8732-e8b5-44e2-9612-1397b72c8687","_uuid":"b18a0656e095287a27442f8888f0bcf327682aeb","trusted":false,"collapsed":true},"cell_type":"code","source":"eval_metric(glove_history, 'acc')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"16994fa6-9c11-4d51-afe6-02d6065bc50e","_uuid":"58ffb6fc61e6117fbd7267d43702e5016959fd8c","trusted":false,"collapsed":true},"cell_type":"code","source":"emb_results2 = test_model(emb_model2, X_train_seq_trunc, y_train_oh, X_test_seq_trunc, y_test_oh, 3)\nprint('/n')\nprint('Test accuracy of word embedding model 2: {0:.2f}%'.format(emb_results2[1]*100))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1c8cefad-3b74-488e-a324-effaa567daaf","_uuid":"8d3f8d489dd78323246b0814c75e7b081ebbe0ae"},"cell_type":"markdown","source":"This result is very close to the model with 8-dimensional word embeddings. So there is no strong improvement. \n\n# Conclusion\nThe best result is achieved with 8-dimensional word embeddings that are trained on the available data. This even outperforms the use of word embeddings that were trained on a much larger Twitter corpus. \n\nUntil now we have just put a Dense layer on the flattened embeddings. By doing this, we do not take into account the relationships between the words in the tweet. This can be done with a recurrent neural network or a 1D convolutional network, which I'll cover in another kernel.\n"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"mimetype":"text/x-python","codemirror_mode":{"version":3,"name":"ipython"},"pygments_lexer":"ipython3","version":"3.6.4","name":"python","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}