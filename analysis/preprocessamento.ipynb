{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import hunspell\n",
    "spellchecker = hunspell.HunSpell('/usr/share/hunspell/en_US.dic',\n",
    "                            '/usr/share/hunspell/en_US.aff')\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"lexicons/positive-words.txt\") as file:\n",
    "    positiveList = set(file.read().splitlines())\n",
    "    \n",
    "with open(\"lexicons/negative-words.txt\") as file:\n",
    "    negativeList = set(file.read().splitlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessing:\n",
    "    def __init__(self, df, filepath):\n",
    "        self.filepath = filepath\n",
    "        self.df = df\n",
    "        self.df.drop(labels=['username',\n",
    "                      'user_handle',\n",
    "                      'date',\n",
    "                      'retweets',\n",
    "                      'favorites',\n",
    "                      'geological_location',\n",
    "                      'mentions',\n",
    "                      'hashtags',\n",
    "                      'tweet_id',\n",
    "                      'permalink',\n",
    "                      'col1', 'col2', 'col3'], axis=1, inplace=True)\n",
    "\n",
    "    # feature 3\n",
    "    def countPositiveCapitalized(self, tokens):\n",
    "        \"\"\"\n",
    "        Calculates the number of positive words that are capitalized\n",
    "        \n",
    "        @params:\n",
    "            tokens: The non stopwords list\n",
    "        \"\"\"\n",
    "        ######print('countPositiveCapitalized()')\n",
    "        counter = 0\n",
    "        tokensSplit = tokens.split()\n",
    "        for t in tokensSplit:\n",
    "            if t[0].isupper() == True and t in positiveList:\n",
    "                counter += 1\n",
    "        return counter\n",
    "\n",
    "    # feature 4\n",
    "    def countNegativeCapitalized(self, tokens):\n",
    "        \"\"\"\n",
    "        Calculates the number of negative words that are capitalized\n",
    "        \n",
    "        @params:\n",
    "            tokens: The non stopwords list\n",
    "        \"\"\"\n",
    "        #####print('countNegativeCapitalized()')\n",
    "        counter = 0\n",
    "        tokensSplit = tokens.split()\n",
    "        for t in tokensSplit:\n",
    "            if t[0].isupper() == True and t in negativeList:\n",
    "                counter += 1\n",
    "        return counter\n",
    "\n",
    "    # feature 5\n",
    "    def hasCapitalized(self, tokens):\n",
    "        \"\"\"\n",
    "        Check if the tweet has capitalized words\n",
    "        \n",
    "        @params:\n",
    "            tokens: The non stopwords list\n",
    "        \"\"\"\n",
    "        #####print('hasCapitalized()')\n",
    "        counter = 0\n",
    "        tokensSplit = tokens.split()\n",
    "        for t in tokensSplit:\n",
    "            if t[0].isupper() == True:\n",
    "                return 1\n",
    "        return 0\n",
    "\n",
    "    # feature 6\n",
    "    def countHashtags(self, tokens):\n",
    "        \"\"\"\n",
    "        Count the number of words that starts with # (hashtags)\n",
    "        \n",
    "        @params:\n",
    "            tokens: The non stopwords list\n",
    "        \"\"\"\n",
    "\n",
    "        #####print('countHashtags()')\n",
    "        counter = 0\n",
    "        for t in tokens:\n",
    "            if t.startswith(\"#\"):\n",
    "                counter += 1\n",
    "        return counter\n",
    "\n",
    "    # feature 7\n",
    "    def countPositive(self, tokens):\n",
    "        \"\"\"\n",
    "        Calculates the number of words that are in the positive words list\n",
    "        \n",
    "        @params:\n",
    "            tokens: The non stopwords list\n",
    "        \"\"\"\n",
    "\n",
    "        #####print('countPositive()')\n",
    "        counter = 0\n",
    "        tokensSplit = tokens.split()\n",
    "        for t in tokensSplit:\n",
    "            if t.lower() in positiveList:\n",
    "                counter += 1\n",
    "        return counter\n",
    "\n",
    "    # feature 8\n",
    "    def countNegative(self, tokens):\n",
    "        \"\"\"\n",
    "        Calculates the number of words that are in the negative words list\n",
    "        \n",
    "        @params:\n",
    "            tokens: The non stopwords list\n",
    "        \"\"\"\n",
    "\n",
    "        #####print('countNegative()')\n",
    "        counter = 0\n",
    "        tokensSplit = tokens.split()\n",
    "        for t in tokensSplit:\n",
    "            if t.lower() in negativeList:\n",
    "                counter += 1\n",
    "        return counter\n",
    "\n",
    "    # feature 9\n",
    "    def countNeutral(self, tokens):\n",
    "        \"\"\"\n",
    "        Calculates the number of words that are in the neutral words list\n",
    "        \n",
    "        @params:\n",
    "            tokens: The non stopwords list\n",
    "        \"\"\"\n",
    "\n",
    "        #####print('countNeutral()')\n",
    "        counter = 0\n",
    "        tokensSplit = tokens.split()\n",
    "        for t in tokensSplit:\n",
    "            if t.lower() not in negativeList and t.lower() not in positiveList:\n",
    "                counter += 1\n",
    "        return counter\n",
    "\n",
    "    # feature 10\n",
    "    def countCapitalizedWords(self, tokens):\n",
    "        \"\"\"\n",
    "        Calculates the number of words that are capitalized\n",
    "        \n",
    "        @params:\n",
    "            tokens: The non stopwords list\n",
    "        \"\"\"\n",
    "\n",
    "        #####print('countCapitalizedWords()')\n",
    "        counter = 0\n",
    "        tokensSplit = tokens.split()\n",
    "        for t in tokensSplit:\n",
    "            if t.isupper() and len(t) > 1:\n",
    "                counter += 1\n",
    "        return counter\n",
    "\n",
    "    # feature 11\n",
    "    def countSpecialCharacters(self, tokens):\n",
    "        \"\"\"\n",
    "        Calculates the number of occurrencies of all special character\n",
    "        \n",
    "        @params:\n",
    "            tokens: The non stopwords list\n",
    "        \"\"\"\n",
    "\n",
    "        #####print('countSpecialCharacters()')\n",
    "        counter = 0\n",
    "        tokensSplit = tokens.split()\n",
    "        for t in tokensSplit:\n",
    "            if not re.match(\"^[a-zA-Z0-9_]*$\", t):\n",
    "                counter += 1\n",
    "        return counter\n",
    "\n",
    "    def countSpecificSpecialCharacter(self, specialCharacter, tokens):\n",
    "        \"\"\"\n",
    "        Calculates the number of occurrencies of a specific special character\n",
    "        \n",
    "        @params:\n",
    "            tokens: The non stopwords list\n",
    "        \"\"\"\n",
    "\n",
    "        #####print('countSpecificSpecialCharacter()')\n",
    "        counter = 0\n",
    "        tokensSplit = tokens.split()\n",
    "        for t in tokensSplit:\n",
    "            if t == specialCharacter:\n",
    "                counter += 1\n",
    "        return counter\n",
    "\n",
    "    def fixSpelling(self, tokens):\n",
    "        #####print('fixSpelling()')\n",
    "\n",
    "                            \n",
    "        words = tokens.split()              \n",
    "        newWords = \"\"#list()\n",
    "        for w in words:\n",
    "            if not spellchecker.spell(w):\n",
    "    #             newWords.append(spellchecker.suggest(w)[0])\n",
    "                try:\n",
    "                    newWords += \" \" + spellchecker.suggest(w)[0]\n",
    "                except(IndexError):\n",
    "                    newWords += \" \" + \"\"\n",
    "            else:\n",
    "    #             newWords.append(w)\n",
    "                newWords += \" \" + w\n",
    "        return newWords\n",
    "            \n",
    "    def stemming(self, tokens):\n",
    "        '''\n",
    "        Apply stemming to each token\n",
    "        '''\n",
    "        \n",
    "        stemmer = SnowballStemmer(\"english\")  \n",
    "        stemmed = [stemmer.stem(w) for w in tokens.split()]\n",
    "        return stemmed\n",
    "\n",
    "    def tweet2words(self, raw_tweet):\n",
    "        \"\"\"\n",
    "        Split the tweet string into words list and remove stopwords\n",
    "            \n",
    "        @params:\n",
    "            raw_tweet: the tweet string collectd\n",
    "        \"\"\"\n",
    "        callout_regex = \"@[A-Za-z0-9_]+\"\n",
    "        \n",
    "        #Remove mencoes a perfis\n",
    "        letters_only = re.sub(callout_regex, \" \", raw_tweet)\n",
    "        letters_only = re.sub(\"[^a-zA-Z]\", \" \", letters_only)\n",
    "        \n",
    "        words = letters_only.lower().split()                             \n",
    "        words = letters_only.split()                             \n",
    "        stops = set(stopwords.words(\"english\"))                  \n",
    "        meaningful_words = [w for w in words if not w in stops] \n",
    "        return( \" \".join( meaningful_words )) \n",
    "\n",
    "    def clean_tweet_length(self, raw_tweet):\n",
    "        \"\"\"\n",
    "        Calculates the number of non stopwords\n",
    "        \n",
    "        @params:\n",
    "            raw_tweet: the tweet string collectd\n",
    "        \"\"\"\n",
    "        letters_only = re.sub(\"[^a-zA-Z]\", \" \",raw_tweet) \n",
    "        words = letters_only.lower().split()                             \n",
    "        stops = set(stopwords.words(\"english\"))                  \n",
    "        meaningful_words = [w for w in words if not w in stops] \n",
    "        return(len(meaningful_words)) \n",
    "\n",
    "    def preprocessing(self):\n",
    "        self.df[\"text\"] = self.df[\"text\"].values.astype(\"U\")\n",
    "        \n",
    "        self.df['tweet2words'] = self.df['text'].apply(self.tweet2words)\n",
    "\n",
    "        self.df[\"num_capitalized\"] = self.df[\"tweet2words\"].apply(self.countCapitalizedWords)\n",
    "\n",
    "        self.df['tweet_length'] = self.df['text'].apply(self.clean_tweet_length)\n",
    "\n",
    "        self.df[\"num_negative_words\"] = self.df[\"tweet2words\"].apply(self.countNegative)\n",
    "        # Number of occurrencies\n",
    "        self.df[\"num_positive_words\"] = self.df['text'].apply(self.countPositive)\n",
    "        self.df[\"num_negative_words\"] = self.df['text'].apply(self.countNegative)\n",
    "        self.df[\"num_neutral_words\"] = self.df['text'].apply(self.countNeutral)\n",
    "\n",
    "        # Capitalized words\n",
    "        self.df[\"has_capitalized\"] = self.df['text'].apply(self.hasCapitalized)\n",
    "        self.df[\"num_capitalised_positive_words\"] = self.df['text'].apply(self.countPositiveCapitalized)\n",
    "        self.df[\"num_capitalised_negative_words\"] = self.df['text'].apply(self.countNegativeCapitalized)\n",
    "\n",
    "\n",
    "        self.df[\"num_hashtags\"] = self.df['text'].apply(self.countHashtags)\n",
    "        self.df[\"num_special_character\"] = self.df['text'].apply(self.countSpecialCharacters)\n",
    "        self.df['correctedText'] =  self.df['tweet2words'].apply(self.fixSpelling)\n",
    "\n",
    "    def exportDataframe(self):\n",
    "        self.df.to_csv(self.filepath + '_preprocessed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countPositiveCapitalized(tokens):\n",
    "    \"\"\"\n",
    "    Calculates the number of positive words that are capitalized\n",
    "\n",
    "    @params:\n",
    "        tokens: The non stopwords list\n",
    "    \"\"\"\n",
    "    ######print('countPositiveCapitalized()')\n",
    "    counter = 0\n",
    "    tokensSplit = tokens.split()\n",
    "    for t in tokensSplit:\n",
    "        if t[0].isupper() == True and t in positiveList:\n",
    "            counter += 1\n",
    "    return counter\n",
    "\n",
    "# feature 4\n",
    "def countNegativeCapitalized(tokens):\n",
    "    \"\"\"\n",
    "    Calculates the number of negative words that are capitalized\n",
    "\n",
    "    @params:\n",
    "        tokens: The non stopwords list\n",
    "    \"\"\"\n",
    "    #####print('countNegativeCapitalized()')\n",
    "    counter = 0\n",
    "    tokensSplit = tokens.split()\n",
    "    for t in tokensSplit:\n",
    "        if t[0].isupper() == True and t in negativeList:\n",
    "            counter += 1\n",
    "    return counter\n",
    "\n",
    "# feature 5\n",
    "def hasCapitalized(tokens):\n",
    "    \"\"\"\n",
    "    Check if the tweet has capitalized words\n",
    "\n",
    "    @params:\n",
    "        tokens: The non stopwords list\n",
    "    \"\"\"\n",
    "    #####print('hasCapitalized()')\n",
    "    counter = 0\n",
    "    tokensSplit = tokens.split()\n",
    "    for t in tokensSplit:\n",
    "        if t[0].isupper() == True:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "# feature 6\n",
    "def countHashtags(tokens):\n",
    "    \"\"\"\n",
    "    Count the number of words that starts with # (hashtags)\n",
    "\n",
    "    @params:\n",
    "        tokens: The non stopwords list\n",
    "    \"\"\"\n",
    "\n",
    "    #####print('countHashtags()')\n",
    "    counter = 0\n",
    "    for t in tokens:\n",
    "        if t.startswith(\"#\"):\n",
    "            counter += 1\n",
    "    return counter\n",
    "\n",
    "# feature 7\n",
    "def countPositive(tokens):\n",
    "    \"\"\"\n",
    "    Calculates the number of words that are in the positive words list\n",
    "\n",
    "    @params:\n",
    "        tokens: The non stopwords list\n",
    "    \"\"\"\n",
    "\n",
    "    #####print('countPositive()')\n",
    "    counter = 0\n",
    "    tokensSplit = tokens.split()\n",
    "    for t in tokensSplitit:\n",
    "        if t.lower() in positiveList:\n",
    "            counter += 1\n",
    "    return counter\n",
    "\n",
    "# feature 8\n",
    "def countNegative(tokens):\n",
    "    \"\"\"\n",
    "    Calculates the number of words that are in the negative words list\n",
    "\n",
    "    @params:\n",
    "        tokens: The non stopwords list\n",
    "    \"\"\"\n",
    "\n",
    "    #####print('countNegative()')\n",
    "    counter = 0\n",
    "    tokensSplit = tokens.split()\n",
    "    for t in tokensSplit:\n",
    "        if t.lower() in negativeList:\n",
    "            counter += 1\n",
    "    return counter\n",
    "\n",
    "# feature 9\n",
    "def countNeutral(tokens):\n",
    "    \"\"\"\n",
    "    Calculates the number of words that are in the neutral words list\n",
    "\n",
    "    @params:\n",
    "        tokens: The non stopwords list\n",
    "    \"\"\"\n",
    "\n",
    "    #####print('countNeutral()')\n",
    "    counter = 0\n",
    "    tokensSplit = tokens.split()\n",
    "    for t in tokensSplit:\n",
    "        if t.lower() not in negativeList and t.lower() not in positiveList:\n",
    "            counter += 1\n",
    "    return counter\n",
    "\n",
    "# feature 10\n",
    "def countCapitalizedWords(tokens):\n",
    "    \"\"\"\n",
    "    Calculates the number of words that are capitalized\n",
    "\n",
    "    @params:\n",
    "        tokens: The non stopwords list\n",
    "    \"\"\"\n",
    "\n",
    "    #####print('countCapitalizedWords()')\n",
    "    counter = 0\n",
    "    tokensSplit = tokens.split()\n",
    "    for t in tokensSplit:\n",
    "        if t.isupper() and len(t) > 1:\n",
    "            counter += 1\n",
    "    return counter\n",
    "\n",
    "# feature 11\n",
    "def countSpecialCharacters(tokens):\n",
    "    \"\"\"\n",
    "    Calculates the number of occurrencies of all special character\n",
    "\n",
    "    @params:\n",
    "        tokens: The non stopwords list\n",
    "    \"\"\"\n",
    "\n",
    "    #####print('countSpecialCharacters()')\n",
    "    counter = 0\n",
    "    tokensSplit = tokens.split()\n",
    "    for t in tokensSplit:\n",
    "        if not re.match(\"^[a-zA-Z0-9_]*$\", t):\n",
    "            counter += 1\n",
    "    return counter\n",
    "\n",
    "def countSpecificSpecialCharacter(specialCharacter, tokens):\n",
    "    \"\"\"\n",
    "    Calculates the number of occurrencies of a specific special character\n",
    "\n",
    "    @params:\n",
    "        tokens: The non stopwords list\n",
    "    \"\"\"\n",
    "\n",
    "    #####print('countSpecificSpecialCharacter()')\n",
    "    counter = 0\n",
    "    tokensSplit = tokens.split()\n",
    "    for t in tokensSplit:\n",
    "        if t == specialCharacter:\n",
    "            counter += 1\n",
    "    return counter\n",
    "\n",
    "def fixSpelling(tokens):\n",
    "    #####print('fixSpelling()')\n",
    "\n",
    "\n",
    "    words = tokens.split()              \n",
    "    newWords = \"\"#list()\n",
    "    for w in words:\n",
    "        if not spellchecker.spell(w):\n",
    "#             newWords.append(spellchecker.suggest(w)[0])\n",
    "            try:\n",
    "                newWords += \" \" + spellchecker.suggest(w)[0]\n",
    "            except(IndexError):\n",
    "                newWords += \" \" + \"\"\n",
    "        else:\n",
    "#             newWords.append(w)\n",
    "            newWords += \" \" + w\n",
    "    return newWords\n",
    "\n",
    "def stemming(tokens):\n",
    "    '''\n",
    "    Apply stemming to each token\n",
    "    '''\n",
    "\n",
    "    stemmer = SnowballStemmer(\"english\")  \n",
    "    stemmed = [stemmer.stem(w) for w in tokens.split()]\n",
    "    return stemmed\n",
    "\n",
    "def tweet2words(raw_tweet):\n",
    "    \"\"\"\n",
    "    Split the tweet string into words list and remove stopwords\n",
    "\n",
    "    @params:\n",
    "        raw_tweet: the tweet string collectd\n",
    "    \"\"\"\n",
    "    callout_regex = \"@[A-Za-z0-9_]+\"\n",
    "\n",
    "    #Remove mencoes a perfis\n",
    "    letters_only = re.sub(callout_regex, \" \", raw_tweet)\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", letters_only)\n",
    "\n",
    "    words = letters_only.lower().split()                             \n",
    "    words = letters_only.split()                             \n",
    "    stops = set(stopwords.words(\"english\"))                  \n",
    "    meaningful_words = [w for w in words if not w in stops] \n",
    "    return( \" \".join( meaningful_words )) \n",
    "\n",
    "def clean_tweet_length(raw_tweet):\n",
    "    \"\"\"\n",
    "    Calculates the number of non stopwords\n",
    "\n",
    "    @params:\n",
    "        raw_tweet: the tweet string collectd\n",
    "    \"\"\"\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \",raw_tweet) \n",
    "    words = letters_only.lower().split()                             \n",
    "    stops = set(stopwords.words(\"english\"))                  \n",
    "    meaningful_words = [w for w in words if not w in stops] \n",
    "    return(len(meaningful_words)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing british_airlines.csv\n"
     ]
    }
   ],
   "source": [
    "tweetsPath = '02_prediction_tweets'\n",
    "onlyfiles = [f for f in listdir(tweetsPath) if isfile(join(tweetsPath, f))]\n",
    "path = onlyfiles[3]\n",
    "# for path in onlyfiles:\n",
    "\n",
    "print(\"Processing \" + path)\n",
    "    # print('prediction_tweets/' + path)\n",
    "df = pd.read_csv('prediction_tweets/' + path, error_bad_lines=False)\n",
    "#     preproc = PreProcessing(df, 'prediction_tweets/' + path.split('.')[0])\n",
    "#     preproc.preprocessing()\n",
    "#     preproc.exportDataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>user_handle</th>\n",
       "      <th>date</th>\n",
       "      <th>retweets</th>\n",
       "      <th>favorites</th>\n",
       "      <th>text</th>\n",
       "      <th>geological_location</th>\n",
       "      <th>mentions</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>...</th>\n",
       "      <th>tweet_length</th>\n",
       "      <th>num_negative_words</th>\n",
       "      <th>num_positive_words</th>\n",
       "      <th>num_neutral_words</th>\n",
       "      <th>has_capitalized</th>\n",
       "      <th>num_capitalised_positive_words</th>\n",
       "      <th>num_capitalised_negative_words</th>\n",
       "      <th>num_hashtags</th>\n",
       "      <th>num_special_character</th>\n",
       "      <th>correctedText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Frank Gardner</td>\n",
       "      <td>FrankRGardner</td>\n",
       "      <td>2018-08-06 18:59</td>\n",
       "      <td>2</td>\n",
       "      <td>36</td>\n",
       "      <td>Your crew were calm, professional, resourceful...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>Your crew calm professional resourceful treme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Frank Gardner</td>\n",
       "      <td>FrankRGardner</td>\n",
       "      <td>2018-08-06 18:41</td>\n",
       "      <td>101</td>\n",
       "      <td>1092</td>\n",
       "      <td>Big thank-u to the lovely crew of @British_Air...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@British_Airways @HeathrowAirport</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>Big thank u lovely crew flight BA today found...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        username    user_handle              date retweets favorites  \\\n",
       "0  Frank Gardner  FrankRGardner  2018-08-06 18:59        2        36   \n",
       "1  Frank Gardner  FrankRGardner  2018-08-06 18:41      101      1092   \n",
       "\n",
       "                                                text  geological_location  \\\n",
       "0  Your crew were calm, professional, resourceful...                  0.0   \n",
       "1  Big thank-u to the lovely crew of @British_Air...                  0.0   \n",
       "\n",
       "  mentions                           hashtags tweet_id  \\\n",
       "0      NaN                                NaN      NaN   \n",
       "1      NaN  @British_Airways @HeathrowAirport      NaN   \n",
       "\n",
       "                         ...                         tweet_length  \\\n",
       "0                        ...                                    6   \n",
       "1                        ...                                   17   \n",
       "\n",
       "  num_negative_words num_positive_words num_neutral_words  has_capitalized  \\\n",
       "0                  0                  1                11                1   \n",
       "1                  1                  2                24                1   \n",
       "\n",
       "  num_capitalised_positive_words num_capitalised_negative_words num_hashtags  \\\n",
       "0                              0                              0            0   \n",
       "1                              0                              0            0   \n",
       "\n",
       "   num_special_character                                      correctedText  \n",
       "0                      4   Your crew calm professional resourceful treme...  \n",
       "1                      5   Big thank u lovely crew flight BA today found...  \n",
       "\n",
       "[2 rows x 29 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('prediction_tweets/' + path, error_bad_lines=False)\n",
    "df[\"text\"] = df[\"text\"].values.astype(\"U\")        \n",
    "df['tweet2words'] = df['text'].apply(tweet2words)\n",
    "df[\"num_capitalized\"] = df[\"tweet2words\"].apply(countCapitalizedWords)\n",
    "df['tweet_length'] = df['text'].apply(clean_tweet_length)\n",
    "df[\"num_negative_words\"] = df[\"tweet2words\"].apply(countNegative)\n",
    "df[\"num_positive_words\"] = df['text'].apply(countPositive)\n",
    "df[\"num_negative_words\"] = df['text'].apply(countNegative)\n",
    "df[\"num_neutral_words\"] = df['text'].apply(countNeutral)\n",
    "df[\"has_capitalized\"] = df['text'].apply(hasCapitalized)\n",
    "df[\"num_capitalised_positive_words\"] = df['text'].apply(countPositiveCapitalized)\n",
    "df[\"num_capitalised_negative_words\"] = df['text'].apply(countNegativeCapitalized)\n",
    "df[\"num_hashtags\"] = df['text'].apply(countHashtags)\n",
    "df[\"num_special_character\"] = df['text'].apply(countSpecialCharacters)\n",
    "df['correctedText'] =  df['tweet2words'].apply(fixSpelling)\n",
    "\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(path+'_preprocessed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_MODEL_TO_DISK = 0\n",
    "LOAD_MODEL = 1\n",
    "\n",
    "# GENERAL LIBS\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "%matplotlib inline\n",
    "\n",
    "# SKLEARN\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "#NLTK\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# GENSIM\n",
    "from gensim.sklearn_api import W2VTransformer\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transformer to select a single column from the data frame to perform additional transformations on\n",
    "    Use on text columns in the data\n",
    "    \"\"\"\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def splitString(self, s):\n",
    "        try:\n",
    "            return s.split()\n",
    "        except AttributeError:\n",
    "            return \"\"\n",
    "            \n",
    "    \n",
    "    def transform(self, X):\n",
    "        # Apply the word2vec transformation\n",
    "        a = X[self.key]\n",
    "#         return wordvecs.fit_transform(a)\n",
    "        return a\n",
    "\n",
    "class Senteces(object):\n",
    "    def __init__(self, dirname):\n",
    "        self.dirname = dirname\n",
    " \n",
    "    def __iter__(self):\n",
    "        for fname in os.listdir(self.dirname):\n",
    "            for line in open(os.path.join(self.dirname, fname)):\n",
    "                yield line.split()\n",
    "\n",
    "class NumberSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transformer to select a single column from the data frame to perform additional transformations on\n",
    "    Use on numeric columns in the data\n",
    "    \"\"\"\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[[self.key]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SVC.joblib',\n",
       " 'MLPClassifier.joblib',\n",
       " 'DecisionTreeClassifier.joblib',\n",
       " 'MultinomialNB.joblib']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "modelsPath = 'models/'\n",
    "onlyfiles = [f for f in listdir(modelsPath) if isfile(join(modelsPath, f))]\n",
    "onlyfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestModels = list()\n",
    "for file in onlyfiles:\n",
    "    bestModels.append(joblib.load(modelsPath + file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetsPath = '03_processed/'\n",
    "onlyfiles = [f for f in listdir(tweetsPath) if isfile(join(tweetsPath, f))]\n",
    "# model = bestModels[0]\n",
    "predictions = list()\n",
    "for model in bestModels: \n",
    "    for file in onlyfiles:\n",
    "        newTweets = pd.read_csv(tweetsPath + file)\n",
    "        newTweets.drop(labels=['text', 'tweet2words'], axis = 1, inplace=True)\n",
    "        newTweets['correctedText'] = newTweets[\"correctedText\"].values.astype(\"U\")\n",
    "        newTweets['sentiment'] = model.predict(newTweets)\n",
    "        predictions.append(newTweets)\n",
    "        classifierName = str(model.get_params('classifier')['classifier']).split('(')[0]\n",
    "        newTweets.to_csv('04_output/' + classifierName + '/' + file.split('.')[0] + '_prediction.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetsDF = pd.read_csv(\"preprocessed2.csv\")\n",
    "\n",
    "tweetsDF.drop(labels=[\"Unnamed: 0\",\n",
    "                      \"airline\", \n",
    "                      \"negativereason\", \n",
    "                      \"airline_sentiment_confidence\", \n",
    "                      \"negativereason\",\n",
    "                      \"negativereason_confidence\",\n",
    "                      \"airline_sentiment\",\n",
    "                      \"text\"], axis=1, inplace=True)\n",
    "\n",
    "tweetsDF[\"tweet2words\"] = tweetsDF[\"tweet2words\"].values.astype(\"U\")\n",
    "tweetsDF[\"correctedText\"] = tweetsDF[\"correctedText\"].values.astype(\"U\")\n",
    "tweetsDF.drop(labels=['tweet2words'], axis=1, inplace=True)\n",
    "\n",
    "# tweetsDF[\"correctedText\"] = tweetsDF[\"correctedText\"].apply(stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"sentiment\"\n",
    "features = [c for c in tweetsDF.columns.values if c not in [target]]\n",
    "numeric_features =  [c for c in tweetsDF.columns.values if c not in ['tweet2words', 'correctedText', target]]\n",
    "\n",
    "X_train = tweetsDF[features]\n",
    "Y_train = tweetsDF[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# bestModel = clf[0]['estimator'][0]\n",
    "allMeasures = dict()\n",
    "for i in range(len(bestModels)):\n",
    "    model = bestModels[i]\n",
    "    tn, fp, fn, tp = confusion_matrix(model.predict(X_train), Y_train).ravel()\n",
    "\n",
    "    measures = dict()\n",
    "    measures[\"acc\"] = (tp + tn)/(tn + fp + fn + tp) * 100\n",
    "    prec = tp/(tp + fp) * 100\n",
    "    recall = tp/(tp + fn) * 100\n",
    "    measures[\"prec\"] = prec\n",
    "    measures[\"recall_sens\"] = recall\n",
    "    measures[\"f1_score\"] = (2 * prec * recall/(prec + recall))\n",
    "    measures['miss_rate'] = (fp + fn) / float(tp + tn + fp + fn) * 100\n",
    "    measures['spec'] = tn/float(tn + fp) * 100\n",
    "    measures['fp_rate'] = fp/float(tn + fp) * 100\n",
    "    allMeasures[i] = measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = pd.DataFrame.from_dict(data=allMeasures, orient='index')\n",
    "performance.set_axis(labels=['SVC', 'DecisionTreeClassifier', 'MLPClassifier', 'MultinomialNB'], inplace=True)\n",
    "# performance.set_axis(labels=['NB'], inplace=True)\n",
    "performance.to_excel(\"performances.xls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetsDF = pd.read_csv(\"Tweets.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5370\n",
      "14168\n",
      "5903\n",
      "8243\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "for d in predictions[:4]:\n",
    "    print(len(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
